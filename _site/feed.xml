<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Scott&#39;s Blog</title>
    <description>I am a graduate student at UW Madison studying computing applications to physical geography and paleoecological change.
</description>
    <link>/</link>
    <atom:link href="/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Fri, 02 Sep 2016 13:59:30 -0500</pubDate>
    <lastBuildDate>Fri, 02 Sep 2016 13:59:30 -0500</lastBuildDate>
    <generator>Jekyll v3.1.6</generator>
    
      <item>
        <title>Ecological Data: Is it &#39;Big Data&#39;?</title>
        <description>&lt;p&gt;We’ve all heard the term ‘Big Data’, though it’s often thrown around as a techy buzzword, along with others, like ‘The Cloud’, without a clear meaning.  In the Williams Lab, we’re working with datasets that are sometimes called ‘Big Data’ in talks by &lt;a href=&quot;https://twitter.com/iceageecologist&quot;&gt;@iceageecolgist&lt;/a&gt; and others, housed in databases like &lt;a href=&quot;http://neotomadb.org&quot;&gt;Neotoma&lt;/a&gt;, &lt;a href=&quot;http://gbif.org&quot;&gt;the Global Biodiversity Information Facility&lt;/a&gt;, and the &lt;a href=&quot;http://paleobiodb.org&quot;&gt;Paleobiology Database&lt;/a&gt;.  Today, I ask, what characteristics of our data make it ‘Big Data’?&lt;/p&gt;

&lt;h3 id=&quot;problem-and-scope&quot;&gt;Problem and Scope&lt;/h3&gt;
&lt;blockquote&gt;
  &lt;p&gt;Question: Can ecological biodiversity data fit under the rubric of Big Data? If so, what are the characteristics that make it Big?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;First, let’s put a limit on the scope of the problem.  Ecology generally has many different subfields, each with their own data and data types.  Some of these may be particularly large, as in the case of ecological modelers, and some may be smaller.  For the sake of argument today, I’ll limit the discussion to ecological biodiversity data documenting &lt;em&gt;occurrences&lt;/em&gt;.   Each occurrence comes with metadata describing what species (typically, but could also be to another taxonomic grouping) was encountered, where it was encountered, and when it was encountered. This type of data is pervasive in the field, and can be used in a host of analyses, including modeling, climate change assessment, and hypotheses testing. Recently, there have been large international campaigns to aggregate these records into large, structured databases that facilitate global biodiversity syntheses.  Three that are commonly encountered are Neotoma (Quaternary), GBIF (modern and instrumental period), and PBDB (deep time).  Since PBDB’s &lt;code class=&quot;highlighter-rouge&quot;&gt;R&lt;/code&gt; package was hard to use, I investigate the question today using data from Neotoma and GBIF.&lt;/p&gt;

&lt;h3 id=&quot;definitions-of-big-data&quot;&gt;Definitions of Big Data&lt;/h3&gt;
&lt;p&gt;There are two often-encountered, decidedly non-technical, designations of Big Data.  The first comes from Wikipedia&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Big data is a term for data sets that are so large or complex that traditional data processing applications are inadequate.&lt;/p&gt;

  &lt;p&gt;(Wikipedia)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This is commonly seen in the marketing materials surrounding big computation and the cloud, though it’s really not a definition at all.  It doesn’t say much about what it is, just that ‘traditional’ means are not capable of processing it, pointing towards distributed computing, cloud computing, and other recent technological advances as its facilitator.  We do get a couple things from this definition though. We know that we’re looking at discrete data sets, partitioned, presumably, in a logical manner.  We’re looking for data sets that ‘traditional’ data processing applications are not feasible.  By using these words, ‘large’ and ‘traditional’, in particular, we can see that ‘Big Data’ is in the eye of the beholder, so to speak, and it depends on your tradition of data processing whether a new dataset is Big or not.  Guterman (2009) suggests, “for some organizations, facing hundreds of gigabytes of data for the first time may trigger a need to reconsider data management options. For others, it may take tens or hundreds of terabytes before data size becomes a significant consideration.”  From Guterman’s perspective, the focus is really on the number of bytes a dataset has, but as we’ll see in a minute, there can be other important factors that comprise a data set’s Bigness.&lt;/p&gt;

&lt;p&gt;The second defintion comes from Yang and Huang’s 2013 book Spatial Cloud Computing:
&amp;gt;Big Data refers to the four V’s: volume, velocity, veracity, and variety.
&amp;gt;
&amp;gt;(Yang and Huang, 2013)&lt;/p&gt;

&lt;p&gt;A varient of this definition can be traced back to an early IBM report on the topic, and can be seen in a variety of cheesy infographics, &lt;a href=&quot;http://www.ibmbigdatahub.com/infographic/four-vs-big-data&quot;&gt;like this one&lt;/a&gt;.  Yang and Huang go on to further describe the meaning of the four V’s, noting that “volume refers to the size of the data; velocity indicates that big data are sensitive to time, variety means big data comprise various types of data with complicated relationships, and veracity indicates the trustworthiness of the data” (p 276).  Here we get a bit more structure than the wikipedia definition gives us, and with the two together, we have a pretty good rubric on which to look at biodiversity datasets.&lt;/p&gt;

&lt;h2 id=&quot;evaluation&quot;&gt;Evaluation&lt;/h2&gt;

&lt;h4 id=&quot;wikipedia&quot;&gt;Wikipedia&lt;/h4&gt;
&lt;p&gt;I argue that the very existence of complex relational databases, like GBIF, Neotoma and PBDB, suggest that biodiversity data do fall under the category of Big Data, as the traditional means of analyzing these data are possible anymore.  Of course, ‘complex’ in the context of the wikipedia statement typically refers to the preponderance of unstructured data, like videos and photos, and ‘large’ usually means too big to fit into a computer’s memory and/or storage drives.  From this perspective, our data is not complex, rather it’s stored in really organized relational tables, and fairly small (the entire Neotoma SQL dump can be downloaded at only 43MB).&lt;/p&gt;

&lt;p&gt;But, if we keep in mind that big data can mean different things to different people, then from our perspective in ecology, our data is Big. Consider the complexity of the relationships between different data records, for example. Figure 1 shows the Neotoma relational table structure, and the complicated web of relationships between each entity.  The data is both spatial and temporal, requiring these attributes, which are known to be messy (see “Veracity”), along with sample data and metadata.  Now, consider keeping track of this for tens of thousands (Neotoma) or hundreds of millions (GBIF) or records, among thousands of independent researchers, and we see why non-traditional techniques like these databases have been developed. Further developments, like APIs and R packages, are even more recent developments to further simplify the tasks of accessing, filtering and working with the datasets. No, ecological biodiversity data does not meet the scale and extent of YouTube, Twitter, or Amazon, but it does require new, custom built tools to store, analyze, and use.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.neotomadb.org/uploads/NeotomaDMD.pdf&quot;&gt;&lt;img src=&quot;/assets/bigData/Neotoma_ER.jpg&quot; alt=&quot;Neotoma_ER&quot; /&gt;&lt;/a&gt;
&lt;em&gt;Figure 1: Neotoma’s Relational Table Structure&lt;/em&gt;&lt;/p&gt;

&lt;h4 id=&quot;volume&quot;&gt;Volume&lt;/h4&gt;
&lt;p&gt;Of the four V’s, the one that most comes to mind when considering what is, or is not, Big Data is volume: how much data is there?  As the quote from Guterman (2009) suggests, some experts consider this to be the only factor in determining what makes data Big. Our datasets are not on the scale of billions of hours of YouTube videos or hundreds of billions of Tweets, but the scale of biodiversity data has exploded in recent years, bringing it to a place where the volume alone is challenging to manage.&lt;/p&gt;

&lt;p&gt;Since the late 1990s, biodiversity databases have quickly and decisively increased the amount of data available to ecologists. Consider Figures 2 and 3, tracking the growth in collections of Neotoma and GBIF through time.  In 1990, only 2 of the records now stored in Neotoma were in digitized collections.  Today, there are over 14,000 datasets.  Each dataset is comprised of spatial and temporal metadata, along with one or more samples with data and associated metadata. The growth rate averages out to about 1.4 datasets every single day for over 26 years.  Considering the time, effort, and money that goes into working up a sediment core (or any of the other data types in Neotoma) this is a really impressive growth rate. For an interesting perspective on ecological Big Data’s reliance on blood, sweat, and tears, take a look at this (Blog Post)[https://contemplativemammoth.com/2013/07/10/is-pollen-analysis-dead-paleoecology-in-the-era-of-big-data/] by former Williams Labber Jacquelyn Gill.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/bigData/Neotoma_Growth.png&quot; alt=&quot;Neotoma_Growth&quot; /&gt;
&lt;em&gt;Figure 2: Cumulative number of datasets in Neotoma&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The scale of GBIF is on an entirely different level than Neotoma (perhaps because some of the data gathering challenges faced in getting paleo data don’t apply as strongly to modern data collection). Today, GBIF houses digital records of well over 500 million observations, recorded specimens (both fossil and living), and occurrences noted in the scientific literature. GBIF’s records are largely comprised of museum collections, which allow their digital collection to date back to before 1900. The facility itself was introduced in 1999 and officially launched in 2001.  Since 2001, the facility’s holdings have grown nearly 300%, from about 180 million in 2001 to just shy of 614 million occurrence records today.  Managing 613+ million records and associated metadata, and comping with such a fast growth rate, is, without a doubt, a data management challenge worthy of Big Data classification.  Figure 3 shows the exponential growth in GBIF’s holdings since AD 1500, and Figure 4 is an interactive map showing the changes in spatial distribution of their observed data since the late 1800’s.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/bigData/GBIF_Growth.png&quot; alt=&quot;GBIF_Growth&quot; /&gt;
&lt;em&gt;Figure 4: Exponential growth of occurrence records in GBIF&lt;/em&gt;&lt;/p&gt;

&lt;link rel=&quot;stylesheet&quot; href=&quot;https://unpkg.com/leaflet@0.7.7/dist/leaflet.css&quot; /&gt;

&lt;script src=&quot;https://unpkg.com/leaflet@0.7.7/dist/leaflet.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://code.jquery.com/jquery-3.1.0.slim.min.js&quot; integrity=&quot;sha256-cRpWjoSOw5KcyIOaZNo4i6fZ9tKPhYYb6i5T9RSVJG8=&quot; crossorigin=&quot;anonymous&quot;&gt;&lt;/script&gt;

&lt;div id=&quot;map&quot; style=&quot;height:500px;&quot;&gt;
&lt;/div&gt;
&lt;p&gt;1890&lt;input type=&quot;range&quot; min=&quot;1890&quot; max=&quot;2016&quot; step=&quot;1&quot; style=&quot;width:50%; display:inline-block; vertical-align:middle&quot; id=&quot;gbif_range&quot; /&gt;2016
&lt;script src=&quot;/assets/gbif_map.js&quot;&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 4: Interactive – Spatial Distribution of GBIF Holdings Through Time&lt;/em&gt;&lt;/p&gt;

&lt;h4 id=&quot;variety&quot;&gt;Variety&lt;/h4&gt;
&lt;p&gt;The second characteristic of Big Data in the four V’s framework is the Variety of the data, and its ‘various types with complicated relationships’ (Yang and Huang). Biodiversity data is highly diverse with many very complicated relationships and interrelationships.&lt;/p&gt;

&lt;p&gt;Neotoma’s holdings range from XRF measurements, to geochronologic data, to fossil vertebrates, to modern pollen surface samples.  In total, there are 23 dataset categories in the database, with more being added from time to time. Though it is structured similarly in the database tables, each of these data types comes from a different community of researchers, using different methods and instruments. Figure 5 shows the breakdown of dataset types in the database.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/bigData/Neotoma_types.png&quot; alt=&quot;Neotoma_Record_types&quot; /&gt;
&lt;em&gt;Figure 5: Dataset Type Breakdown of Neotoma’s Holdings&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;GBIF has 9 defined record type categories, including human observation, living specimen, literature review, and machine measurements.  As with the Neotoma dataset types, these are wildly different from each other.  A living specimen is clearly a totally different type of data to work with than something was derived from a literature review. Yet all of these types coexist together in these large biodiversity datasets. Figure 6 shows how GBIF’s records are distributed amongst these nine types.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/bigData/gbif_types.png&quot; alt=&quot;GBIF&quot; /&gt;
&lt;em&gt;Figure 6: Dataset Type Breakdown of GBIF Holdings&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;To further add to the variety and complexity of our data, it is both spatial and temporal in nature, causing complicated interrelationships between data entities. 87.6 % of GBIF’s records are georeferenced to a real place in the world. 100% of Neotoma’s datasets have spatial information. In these databases, the spatial information is compounded by other fields that describe the location of the observation.  For example, Neotoma has fields describing the site where the fossil was found – it’s altitude, environment, area.  PBDB has extensive metadata for depositional environment, giving additional context to fossil occurrences.  GBIF often notes somewhat colloquial location descriptions in addition to geographic coordinates.   And, of course, there are the relationships between the spatial coordinates themselves – are these things in the same place? do they overlap?&lt;/p&gt;

&lt;p&gt;Managing data with a spatial component is nearly always more challenging than managing data without it. Figure 7 shows how the spatial locations of the datasets contained in Neotoma have changed through time.  Note the expansion in Europe and eastern Asia, and the lack of datasets in Africa.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/assets/bigData/neotoma_spatial_dist.png&quot;&gt;&lt;img src=&quot;/assets/bigData/neotoma_spatial_dist.png&quot; alt=&quot;Neotoma_Maps&quot; /&gt;&lt;/a&gt;
&lt;em&gt;Figure 7: Spatial distribution of additions in Neotoma since 1990&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;A final point on variety is that each record, though now cleanly structured and easily accessed as a record in a database, represents the work of an individual researcher.  The controlled vocabularies and organization policies enforced by the databases have helped to efficiently aggregate the data, however, nearly every record was collected, worked up, and published by a unique individual.  Figure 8 shows the number of datasets attributed to each PI in Neotoma.  Yes the names are too small to read.  The point, though, is that while a couple researchers have a very large number of datasets credited to them (John T Andrews has the most with 335), most have many fewer.  The median number of datasets contributed is 2, and the 3rd quartile value is just 7.  Each researcher will use different equipment, in a different way, call things different names, and generally just do things slightly differently – yielding a highly variable dataset.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/assets/bigData/Neotoma_PIs.png&quot;&gt;&lt;img src=&quot;/assets/bigData/Neotoma_PIs.png&quot; alt=&quot;GBIF&quot; /&gt;&lt;/a&gt;
&lt;em&gt;Figure 8: Neotoma dataset submissions by principle investigator&lt;/em&gt;&lt;/p&gt;

&lt;h4 id=&quot;veracity&quot;&gt;Veracity&lt;/h4&gt;
&lt;p&gt;Ecological data has high levels of uncertainty associated with it.  Some can be estimated, like temporal and spatial uncertainty.  Others are less amenable to being quantified, for example inter-researcher identification differences, measurement errors, and data lost in the transition from field to lab to database. See &lt;a href=&quot;http://www.sciencedirect.com/science/article/pii/S0277379116300142#appsec1&quot;&gt;this paper&lt;/a&gt; for a Paleon project that used expert elicitation to quantify the differences between the dates assigned to European settlement horizon, a process they argue varies between sites, and depends on the “temporal density of pollen samples, time-averaging of sediments, the rapidity of forest clearance and landscape transformation, the pollen representation of dominant trees, which can dampen or amplify the ragweed signal, and expert knowledge of the region and the late-Holocene history of the site.” The raw data from the expert elicitation is included as supplementary information in their paper, and can be seen to vary pretty significantly between the four experts.&lt;/p&gt;

&lt;p&gt;Some information will be lost in the process of going from a field site through a lab workflow to being aggregated in the dataset.  Not all process details can be incorporated into database metadata fields, and probably more importantly, contextual details essential to proper interpretation of the data often gets lost on aggregation.&lt;/p&gt;

&lt;p&gt;Coincidentally, when I start working on my PhD here at UW, I’ll be working to tackle some of these uncertainty issues.&lt;/p&gt;

&lt;p&gt;To illustrate the veracity (or lack thereof) of the biodiversity data, let’s look at spatial coordinate uncertainty in GBIF and temporal uncertainty of chronological control points in Neotoma. The GBIF database, in addition to recording the geographic coordiantes of an occurrence, also includes a field for uncertainty in spatial location, though this field is optional.  I downloaded 10,000 records of the genus &lt;em&gt;Picea&lt;/em&gt;, of which over half did not include this field (though all were georeferenced).  This means that even if I am able include and propagate uncertainty in my models (as in Bayesian Hierarchical Models), I would be unable to do so really effectively, because few researchers even report this field. Of the 4,519 records that did report &lt;code class=&quot;highlighter-rouge&quot;&gt;coordinateUncertaintyInMeters&lt;/code&gt;, the average uncertainty was 305m (if you exclude zero, which seems reasonable to do). The maximum uncertainty in this dataset was 1,970m.  From this brief, and admittedly flawed, assessment, we can see there are some pretty serious problems with using the coordinates without considering their uncertainty first.  If, for example, you’re using 800m gridded climate model output to look at environmental covariates to species presence (which I do), a 300m uncertainty in species location could cause significant deviations due to gridcell mis-assignment, particularly in mountainous regions like the Western U.S.&lt;/p&gt;

&lt;p&gt;On the temporal side of things, we can do a similar assessment, this time using the Neotoma data.  Neotoma samples are assigned an age using age controls (like radiocarbon dates or varve counts) or an age model, which interpolates between the age controls. The age model issue is a challenging one, and there’s a lot of literature out there about it, as well as software to improve from simple linear models. Every age model is based on a set of age controls, which often have uncertainty associated with them.   Neotoma records an minimum and maximum age for each age control for each dataset.  Out of a sample of 32,341 age controls in the database, only 5,722 reported age uncertainty.  Some record types, like varves, can perhaps be assigned an uncertainty of zero, so we can safely ignore 2,830 more controls, leaving us with 2,892 that report values for minimum and maximum age. The summary statistics for these age controls suggest that the median age model tie point has a temporal uncertainty of 260.0 years. The 25% percentile is an uncertainty of 137.5 years and the 75% 751.2 years.  Using the mean of 260.0 years, I suggest that we can only identify down to ± 130 years of the actual date.  Considering sediment mixing, laboratory precision, and other processes at work, maybe this isn’t that big of a deal, but it definitely is something to be aware of and contributes to biodiversity data’s lack of absolute veracity.&lt;/p&gt;

&lt;h4 id=&quot;velocity&quot;&gt;Velocity&lt;/h4&gt;
&lt;p&gt;The final piece of the framework is the data’s velocity – how time sensitive is the data.  Data’s velocity important because high velocity data must be analyzed as a stream.  Tweets, for example, must be analyzed for trends as they are posted. Knowing the trending topics of two weeks ago might be interesting to me, but the real draw of a Big Data platform like twitter is that I can participate in the trending topics of &lt;em&gt;right now&lt;/em&gt;.  To do such an analysis, one must use sophisticated sampling techniques and algorithms to detect clusters and trends in real time, for example (this paper)[http://jmlr.csail.mit.edu/proceedings/papers/v17/bifet11a/bifet11a.pdf], which comments on sampling strategies used for trend detection.&lt;/p&gt;

&lt;p&gt;This is the one area where I would suggest that ecological biodiversity data is not Big Data.  Biodiversity analyses, like species distribution models, at least the ones I am familiar with, usually take between a few minutes and a few days to complete and are not especially time sensitive.  The rate of increase in data volume in both Neotoma and GBIF is not fast enough to invalidate the results from previous analyses.  Neotoma gets approximately 1.4 new datasets each day (1990-2016 average).  GBIF gets about 59,000 new occurrences each day (2000-2015 average).  Sure, that’s a lot of new datasets, but the likelihood you would actually use the new data in a given analysis is low, and the likelihood that its immediately inclusion into a new model would significantly change your conclusions is even lower.&lt;/p&gt;

&lt;p&gt;The velocity of data coming into the databases, particularly into GBIF, is staggering, no doubt about it.  Nonetheless, I don’t think it it warrants the use of specialized streaming algorithms for extracting information from the new data points.  I have not seen anyone attempt to do such a thing (though maybe this would be an interesting experiment?).  Moreover, there is little incentive to immediately analyze the data, because there is next to nothing to be gained from modeling biodiversity faster than you can report your results in publications.&lt;/p&gt;

&lt;h3 id=&quot;so-is-it&quot;&gt;So, is it?&lt;/h3&gt;
&lt;p&gt;Velocity notwithstanding, biodiversity occurrence data passes four of five facets of the Big Data, so I conclude that, &lt;strong&gt;yes, it is big data.&lt;/strong&gt; It requires specialized databases and software to interact with it, it has large numbers of records, it is extremely diverse, and it has high levels of uncertainty with which to deal.&lt;/p&gt;

&lt;p&gt;Looking forward, I suspect Big Data will continue to challenge those involved in synthetic research. Perhaps one of the most challenging aspects is the relatively short period of time in which these data became Big. Figures 9 and 10 show the annual increase in holdings for Neotoma (Fig. 9) and GBIF (Fig 10) through time (top) and the rate of change of annual increase (bottom). While Neotoma’s rate of increase as remained relatively steady through time (clear from the near-linear trend in Figure 2), GBIF’s rate shows a significant upward trend in the last several years.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/assets/bigData/Neotoma_growth_diff.png&quot;&gt;&lt;img src=&quot;/assets/bigData/Neotoma_growth_diff.png&quot; alt=&quot;Neotoma_Delta&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 9: Neotoma holdings, Annual Change and Rate of Change of Annual Change&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/assets/bigData/gif_growth_diff.png&quot;&gt;&lt;img src=&quot;/assets/bigData/gif_growth_diff.png&quot; alt=&quot;GBIF_Delta&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Figure 10: GBIF holdings, Annual Change and Rate of Change of Annual Change&lt;/em&gt;&lt;/p&gt;
</description>
        <pubDate>Sun, 21 Aug 2016 08:22:52 -0500</pubDate>
        <link>/research/paleo/2016/08/21/Big-Data-In-Ecology.html</link>
        <guid isPermaLink="true">/research/paleo/2016/08/21/Big-Data-In-Ecology.html</guid>
        
        
        <category>Research</category>
        
        <category>Paleo</category>
        
      </item>
    
      <item>
        <title>Adding Shareable URLs to IceAgeMapper</title>
        <description>&lt;p&gt;One of the features Rob suggested I add to Ice Age Mapper during our last meeting was a dynamic url that would record the current state of the application, and could thus be shared between users. I took a stab at that last week, and got it working pretty well.  I thought it would be a lot of re-coding from the ground up, but it turns out that most of what I had written previously could be easily converted to load a URL string.  My application only generates a shareable URL when the user clicks the ‘Share’ button, but in theory, the app could easily be modified to generate a new URL each time an action was taken.  I think this would actually &lt;strong&gt;&lt;em&gt;Not&lt;/em&gt;&lt;/strong&gt; be a good idea, because it would mean there would be an entry in the user’s web history for each action they took inside of the application, meaning they would have to click the back button like a million times if they messed up.  Good to know support exists for that though.&lt;/p&gt;

&lt;p&gt;Another plus of designing a dynamic state URL is that it can be used to share the current configuration on Twitter, GooglePlus, by email, or other social media.  While not critical for our purposes, it seems like it’s never a bad thing to tap into social channels.&lt;/p&gt;

&lt;p&gt;There are two main parts of implementing a dynamic state url: generating and parsing.  The generating phase includes functions that get the current state of the system and translate them into a URL variable, and then string the URL variables together into a complete URL.  In the parsing phase, the URL variable parse (or not, if they don’t exist) and translate them into function calls to re-generate the desired state.  Before starting to code, make a list of the parts of the state you want to keep track of.  Do you want to keep track of every click made to get to a certain configuration or just the configuration itself?&lt;/p&gt;

&lt;p&gt;I decided I want to keep track of the following parts of the application state:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;em&gt;Taxon&lt;/em&gt;: the data that is currently being displayed, as returned from a Neotoma API call.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Map Center&lt;/em&gt;: Geographic center of current map view, as latitude and longitude.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Map Zoom&lt;/em&gt;: Zoom level of current map view.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Minimum Year&lt;/em&gt;: Minimum (most recent) year in temporal view.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Maximum Year&lt;/em&gt;: Maximum (most distant) year in temporal view.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Panel Configuration&lt;/em&gt;: For each panel, is it open or closed?  Currently, there are three panels: Taxonomy, Site, and NicheViewer.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Layer Configuration&lt;/em&gt;:  For each layer, is it visible or hidden?  Currently, there are three layers: Ice Sheets, Sites, and Heatmap.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;You’ll likely find that there are things you want to add to the state at a later time, but with the general framework, such additions should be really easy.&lt;/p&gt;

&lt;h3 id=&quot;part-1-generation&quot;&gt;Part 1: Generation&lt;/h3&gt;
&lt;p&gt;To generate the URL, I used the &lt;code class=&quot;highlighter-rouge&quot;&gt;URI.js&lt;/code&gt; library.  This &lt;a href=&quot;https://medialize.github.io/URI.js/&quot;&gt;library&lt;/a&gt; makes it easy to parse, add to, and validate URL strings on the current window, or another window.  Generate a new URI for the current window location:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-js&quot; data-lang=&quot;js&quot;&gt;  &lt;span class=&quot;nx&quot;&gt;uri&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;URI&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;And then add query variables as needed, using:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-js&quot; data-lang=&quot;js&quot;&gt;  &lt;span class=&quot;nx&quot;&gt;uri&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;addQuery&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&#39;key&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;value&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The next, and most intensive step in this phase to get the values of each state component at this point in time.  For some, like the leaflet map center and zoom, it will be easy to do this, because the leaflet map object already tracks these for you (&lt;code class=&quot;highlighter-rouge&quot;&gt;map.getCenter()&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;map.getZoom()&lt;/code&gt;).  Depending on your coding style, you may already have pointers to some of the components, or you may need to devise a way of going to get the values.  Because I chose to only generate the new state URL once the user has requested it, we can write some functions to go get the values at the time they click the button. Mostly, though, I use a &lt;code class=&quot;highlighter-rouge&quot;&gt;globals&lt;/code&gt; object, that keeps references to a variety of important properties that I might want to access throughout my code.  I think this is a good compromise between having a ton of global variables floating around, and totally scoping the variables into functions.  Maybe I’m wrong, not sure…&lt;/p&gt;

&lt;p&gt;Anyways, for each of my state components, I go get it’s value, and then set it to our new &lt;code class=&quot;highlighter-rouge&quot;&gt;uri&lt;/code&gt; object.  Remember that the properties should be Boolean, String, or Numeric types, rather than object or something else that can’t be easily serialized into the URI.  This can be a little tricky, but it’s important so think about how you can make it work.  For example, if I want to populate a panel with data, I can’t easily serialize the data into the URI string.  Instead, I tell the URL that I do want to populate that panel, and I want that panel to automatically open.  Then I write re-write the panel function so that it can automatically call Neotoma and populate the details with the API call results. More on that in the next section.&lt;/p&gt;

&lt;p&gt;When the URI component contains all of your desired state components, you can get it’s value by calling &lt;code class=&quot;highlighter-rouge&quot;&gt;uri.toString()&lt;/code&gt;.  If you set &lt;code class=&quot;highlighter-rouge&quot;&gt;window.location.href=uri.toString()&lt;/code&gt; you will reload the page.  If that’s what you want, go for that.  In my case, I set a text box to the value of the &lt;code class=&quot;highlighter-rouge&quot;&gt;toString()&lt;/code&gt; method, which users can copy and paste if they want. In addition, I do add a history entry into the user’s browser history.  This is accomplished by:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-js&quot; data-lang=&quot;js&quot;&gt;  &lt;span class=&quot;nb&quot;&gt;window&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;history&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;pushState&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Ice Age Mapper&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;Ice Age Mapper&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;uri&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;toString&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;That’s about all there is on the generation side of things.  The more involved coding comes when trying to parse a share url.&lt;/p&gt;

&lt;h3 id=&quot;part-2-parsing&quot;&gt;Part 2: Parsing&lt;/h3&gt;
&lt;p&gt;Once you have a URL, you need to put in the infrastructure to generate the state that the URL calls for.  First though, you need to read the url string and parse it into its component parts.  To read the URL, I found that this function was super helpful (I borrowed it from &lt;a href=&quot;http://stackoverflow.com/questions/901115/how-can-i-get-query-string-values-in-javascript&quot;&gt;this StackOverflow post&lt;/a&gt;):&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-javascript&quot; data-lang=&quot;javascript&quot;&gt;  &lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;getURLParameterByName&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;url&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;url&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;url&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;window&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;location&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;href&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
      &lt;span class=&quot;nx&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;replace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sr&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;[\[\]]&lt;/span&gt;&lt;span class=&quot;sr&quot;&gt;/g&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;\\$&amp;amp;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
      &lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;regex&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;RegExp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;[?&amp;amp;]&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;(=([^&amp;amp;#]*)|&amp;amp;|#|$)&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
          &lt;span class=&quot;nx&quot;&gt;results&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;regex&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;exec&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;url&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;results&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;null&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;results&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&#39;&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;decodeURIComponent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;results&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;replace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sr&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\+&lt;/span&gt;&lt;span class=&quot;sr&quot;&gt;/g&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot; &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;hr /&gt;
&lt;p&gt;You can then get the query parameters from the URL string like so:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-javascript&quot; data-lang=&quot;javascript&quot;&gt;  &lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;queryVar&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;getURLParameterByName&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;key&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;For each state component, I parsed the URI variable associated with it.  I also added some checks to make sure that if the query was not in the URI, the application wouldn’t crash, but would instead default to something smart.  For example, to get the currently displayed taxon:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-javascript&quot; data-lang=&quot;javascript&quot;&gt;  &lt;span class=&quot;nx&quot;&gt;taxon&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;getURLParameterByName&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&#39;taxon&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;taxon&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
    &lt;span class=&quot;nx&quot;&gt;globals&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;taxon&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;taxon&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;//set the name in the search box&lt;/span&gt;
    &lt;span class=&quot;nx&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;#searchBar&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;taxon&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;toProperCase&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
    &lt;span class=&quot;nx&quot;&gt;globals&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;autoload&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Once all of the query variables have been parsed, we need a way of translating the new state information into the actual application state.  I do this in two steps.  First, I have a function that does all of the parsing.  During the parsing, the global variable objects gets property values for the configuration (e.g., &lt;code class=&quot;highlighter-rouge&quot;&gt;globals.taxon = &#39;Quercus&#39;&lt;/code&gt;).  Next, I call a load function, which is pretty much the same as what I had when I didn’t allow URL configuration, but instead of just setting the variables to &lt;code class=&quot;highlighter-rouge&quot;&gt;Null&lt;/code&gt; at the start, it checks to see if the property has already been set during the parsing phase.  This method works really well for components like map zoom and time extent.  However, it will not automatically load the data from Neotoma, because loading the data requires a button click to send an AJAX request to the Neotoma API.  Therefore, we add a &lt;code class=&quot;highlighter-rouge&quot;&gt;globals.autoload&lt;/code&gt; property, which automatically triggers a click on that button, if the necessary state configurtion variables (like taxon) are set in the URL.&lt;/p&gt;

&lt;h3 id=&quot;part-3--sharing-on-social-media&quot;&gt;Part 3:  Sharing on Social Media&lt;/h3&gt;
&lt;p&gt;One you’ve implemented the generating and parsing, and know that your share URL gives you a reliable application state representation, you can share the URL on twitter or other social media really easily.&lt;br /&gt;
#### Copying to the clipboard
While not social, you may wish to allow users to copy the link directly to their copy-paste clipboard.  I read some discussion of how this may be a bad idea for security.  I’m not sure – I added it anyways.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-js&quot; data-lang=&quot;js&quot;&gt;&lt;span class=&quot;c1&quot;&gt;// create hidden text element, if it doesn&#39;t already exist&lt;/span&gt;
&lt;span class=&quot;nx&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;#share-link&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;focus&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;//highlight the text element that contains the link&lt;/span&gt;
&lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;succeed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;try&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;nx&quot;&gt;succeed&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;document&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;execCommand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;copy&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;//do the copying&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;catch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
  &lt;span class=&quot;nx&quot;&gt;succeed&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;false&lt;/span&gt;
  &lt;span class=&quot;nx&quot;&gt;console&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;succeed&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;//Boolean&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4 id=&quot;sharing-on-twitter&quot;&gt;Sharing on Twitter&lt;/h4&gt;
&lt;p&gt;Twitter allows you to configure a link that pre-populates a tweet composer with message body, share url, hashtags, mentions, etc.  This was a little hard to get the hang of, and I still don’t think it’s quite right.  I used the &lt;code class=&quot;highlighter-rouge&quot;&gt;URI.js&lt;/code&gt; library again to generate this URL, and then set it to the &lt;code class=&quot;highlighter-rouge&quot;&gt;href&lt;/code&gt; property of a link.  The &lt;code class=&quot;highlighter-rouge&quot;&gt;generateTwitterLink&lt;/code&gt; function is called right after the share URL is generated, so that it is available to the user if they choose to share on twitter. FYI: Even if you have a really long share url, it will only take up 22 characters of your tweet &lt;em&gt;if you are on a real server&lt;/em&gt;.  If you are on localhost, which isn’t a qualified domain, it will take up all of the characters, so might not work.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-js&quot; data-lang=&quot;js&quot;&gt;&lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;generateTwitterLink&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(){&lt;/span&gt;
  &lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;twitterURL&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;URI&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;http://twitter.com/share/&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;//base link&lt;/span&gt;
  &lt;span class=&quot;nx&quot;&gt;twitterURL&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;addQuery&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;url&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;globals&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;shareURI&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;//prepopulate with a URL&lt;/span&gt;
  &lt;span class=&quot;nx&quot;&gt;twitterURL&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;addQuery&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;text&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;Check out my Ice Age Map!&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;//prepopulate text&lt;/span&gt;
  &lt;span class=&quot;nx&quot;&gt;twitterURL&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;addQuery&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;hashtags&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;paleo&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;nx&quot;&gt;twitterURL&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;twitterURL&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;toString&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;//generate the string from the object&lt;/span&gt;
  &lt;span class=&quot;nx&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;.twitter-share-button&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;attr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;href&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;twitterURL&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;//set the link attribute so we actually use the dynamic URL&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4 id=&quot;sharing-with-email&quot;&gt;Sharing with Email&lt;/h4&gt;
&lt;p&gt;Perhaps the most likely way to share an application state for this application is by email, so I added a method that you can easily email the link out to your collaborators from inside the app. This is super easy, you just need to set the subject and body of the &lt;code class=&quot;highlighter-rouge&quot;&gt;mailto:&lt;/code&gt; string inside of the link &lt;code class=&quot;highlighter-rouge&quot;&gt;href&lt;/code&gt;. Since we don’t know who to send it to, we leave the &lt;code class=&quot;highlighter-rouge&quot;&gt;to:&lt;/code&gt; field blank.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-js&quot; data-lang=&quot;js&quot;&gt;&lt;span class=&quot;nx&quot;&gt;link&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;mailto:?to=&amp;amp;&quot;&lt;/span&gt;
&lt;span class=&quot;nx&quot;&gt;link&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;subject=&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;encodeURIComponent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Ice Age Mapper&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;//changes spaces to %20, etc&lt;/span&gt;
&lt;span class=&quot;nx&quot;&gt;link&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&amp;amp;body=&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;shareURI&lt;/span&gt;
&lt;span class=&quot;nx&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;#emailLink&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&#39;href&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;link&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4 id=&quot;sharing-on-google&quot;&gt;Sharing on Google+&lt;/h4&gt;
&lt;p&gt;Sharing on Google+, which I don’t know if anyone actually uses – I don’t–  was really, really easy. Assuming you have the required script included, you can have your sharing element be something like:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-html&quot; data-lang=&quot;html&quot;&gt;div class=&quot;g-plus&quot; data-action=&quot;share&quot;&amp;gt;&lt;span class=&quot;nt&quot;&gt;&amp;lt;/div&amp;gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Then you can enable the sharing with your URL by setting the url data attribute inside of your javascript code, again, immediately after you generate the share URL.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-js&quot; data-lang=&quot;js&quot;&gt;&lt;span class=&quot;nx&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;.g-plus&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&#39;href&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;shareURI&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

</description>
        <pubDate>Sun, 21 Aug 2016 08:22:52 -0500</pubDate>
        <link>/research/paleo/2016/08/21/Adding-Shareable-URLs-To-IceAgeMapper.html</link>
        <guid isPermaLink="true">/research/paleo/2016/08/21/Adding-Shareable-URLs-To-IceAgeMapper.html</guid>
        
        
        <category>Research</category>
        
        <category>Paleo</category>
        
      </item>
    
      <item>
        <title>Updating R on Debian Linux</title>
        <description>&lt;p&gt;Why the &lt;code class=&quot;highlighter-rouge&quot;&gt;apt-get&lt;/code&gt; package manager doesn’t contain the latest version of &lt;code class=&quot;highlighter-rouge&quot;&gt;R&lt;/code&gt; automatically, I’m not sure. I recently realized I have been downloading a 2+ year old distribution for all of my SDM timing runs by running the standard &lt;code class=&quot;highlighter-rouge&quot;&gt;sudo apt-get install r-base&lt;/code&gt; command at the shell.  For several weeks, this was fine, but today the package &lt;code class=&quot;highlighter-rouge&quot;&gt;Rcpp&lt;/code&gt;, which wraps compiled C++ code in the R environment failed to compile.  I spent most of the afternoon trying to figure out what was going on.  I didn’t even occur to me that the  &lt;code class=&quot;highlighter-rouge&quot;&gt;r-base&lt;/code&gt; package I was using was the root cause.&lt;/p&gt;

&lt;p&gt;It is not easy to figure out how to update the core R package, but, like most things in linux, it comes down to a correctly ordered set of calls to a package manager.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Note:&lt;/em&gt; I am using a Debian 8 Jessie image, version v20160718&lt;/p&gt;

&lt;p&gt;###Steps:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo apt-get remove r-base&lt;/code&gt;.  Remove the old version of R.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo nano /etc/apt/sources.list&lt;/code&gt;.  This file holds all of the package repositories for &lt;code class=&quot;highlighter-rouge&quot;&gt;apt-get&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;Inside of it, copy and paste:
    &lt;pre&gt;
deb http://cran.rstudio.com/bin/linux/debian jessie-cran3/
&lt;/pre&gt;

    &lt;p&gt;This tells the manager to look in this repository for a copy of the R distribution.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Save and close the text editor.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;At the shell, type:
  &lt;code&gt;
  gpg --keyserver keyserver.ubuntu.com --recv-key E084DAB9
  &lt;/code&gt;
  and then
  &lt;code&gt;
  gpg -a --export E084DAB9 | sudo apt-key add -
  &lt;/code&gt;
  What does this do? I’m not exactly sure, but I think it has to do with the package integrity checks down when downloading things from a package manager.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo apt-get update&lt;/code&gt;.  Update the installed packages.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo apt-get install r-base&lt;/code&gt;.  Install the core R functionality, hopefully this time using the newest version.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sudo apt-get install r-base-dev&lt;/code&gt;.  Install the development headers to allow packages that are not in debian repositories.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;At this point, you should have a newly updated R version.  You can check with R.version.  For me, this worked for updating from R version &lt;code class=&quot;highlighter-rouge&quot;&gt;3.0.1&lt;/code&gt; to R version &lt;code class=&quot;highlighter-rouge&quot;&gt;3.3.1&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;If you have package install error, it’s definitely worth checking if an update in the r-base package could be responsible.&lt;/p&gt;
</description>
        <pubDate>Tue, 19 Jul 2016 09:22:52 -0500</pubDate>
        <link>/research/cloudcomputing/2016/07/19/Updating-R-on-Debian.html</link>
        <guid isPermaLink="true">/research/cloudcomputing/2016/07/19/Updating-R-on-Debian.html</guid>
        
        
        <category>Research</category>
        
        <category>CloudComputing</category>
        
      </item>
    
      <item>
        <title>Preliminary Thesis Results: Update 6/26</title>
        <description>&lt;p&gt;I’ve made it through 4,830 of the experiments I want to run for my thesis, so I’m taking this opportunity to reflect on the preliminary results that I have so far, visually check what I have so far, and make any necessary changes before doing the more expensive portion of the experiments.  So far, the results look okay, but definitely not what I expected.  The effect of the computing configuration on computing time seems to be minimal.  On the other hand, the effect of different experimental parameters is pretty significant.&lt;/p&gt;

&lt;h3 id=&quot;status&quot;&gt;Status&lt;/h3&gt;

&lt;h4 id=&quot;completion-statistics&quot;&gt;Completion Statistics&lt;/h4&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Property&lt;/th&gt;
      &lt;th&gt;Number Recored&lt;/th&gt;
      &lt;th&gt;Percentage of Completed&lt;/th&gt;
      &lt;th&gt;Percentage of Total&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Completed&lt;/td&gt;
      &lt;td&gt;4830&lt;/td&gt;
      &lt;td&gt;92.18%&lt;/td&gt;
      &lt;td&gt;5.72%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Error&lt;/td&gt;
      &lt;td&gt;410&lt;/td&gt;
      &lt;td&gt;7.82%&lt;/td&gt;
      &lt;td&gt;0.05%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Not Started&lt;/td&gt;
      &lt;td&gt;59550&lt;/td&gt;
      &lt;td&gt;0%&lt;/td&gt;
      &lt;td&gt;70.5 %&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Removed*&lt;/td&gt;
      &lt;td&gt;19680&lt;/td&gt;
      &lt;td&gt;0%&lt;/td&gt;
      &lt;td&gt;23.3%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Total&lt;/td&gt;
      &lt;td&gt;84470&lt;/td&gt;
      &lt;td&gt;–%&lt;/td&gt;
      &lt;td&gt;100%&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Removed experiments are those that specify fitting the BRT model with 10,000 training examples, which takes too long to be practical.  Instead, these were replaced with the &lt;em&gt;nSensitivity&lt;/em&gt; experiment series, which tests the computing time sensitivity to different numbers of input points.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/configs.png&quot; alt=&quot;Image&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;inspection-of-results&quot;&gt;Inspection of Results&lt;/h3&gt;
&lt;p&gt;One of the main problems I’m having in interpreting the results is that there are four separate experimental variables, which makes it difficult to properly interpret the influence of only one variable.  Of course, I want to isolate the effect of the computing memory and virtual cores.&lt;/p&gt;

&lt;h4 id=&quot;influence-of-additional-cores&quot;&gt;Influence of Additional Cores&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/assets/cores_totalTime.png&quot; alt=&quot;Cores&quot; /&gt;
If we plot vCPU vs. total time, there is no clear relationship.  The major spike at cores = 4 is due to the fact that I used a four core virtual machine to test the effect of different numbers of training examples.  These tests are not part of the experiments that I’m doing on every machine. If we remove these extraneous points, and treat them separately later, we can fit a linear model that shows a very slightly decreasing slope:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y=136.103 - 4.548Cores&lt;/script&gt;

&lt;h4 id=&quot;influence-of-additional-memory&quot;&gt;Influence of Additional Memory&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/assets/mem_totalTime.png&quot; alt=&quot;Memory&quot; /&gt;
The effect of adding additional memory is slightly more clearly linear and decreasing than the effect of adding addition CPU cores, although it is still not particularly steep.  The linear model here takes the form:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y = 134.87 - 1.26GB&lt;/script&gt;

&lt;h4 id=&quot;influence-of-spatial-resolution&quot;&gt;Influence of Spatial Resolution&lt;/h4&gt;
&lt;p&gt;As expected, higher resolution outputs take longer to process than their lower resolution counterparts. Because increasing spatial resolution results in an exponential number of cells, a linear model is not particularly well suited to this application.  An exponentially decreasing relationship can be seen in the prediction time.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/spatial_resolution.png&quot; alt=&quot;Spatial Resolution&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The relationship between spatial resolution takes the form:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y = 145.27 - 52.78degree&lt;/script&gt;

&lt;h4 id=&quot;influence-of-training-examples&quot;&gt;Influence of Training Examples&lt;/h4&gt;
&lt;p&gt;The clearest relationship in all of the experimental variables is between total model time and number of training examples.  This relationship is clearly monotonically increasing, perhaps at a rate slightly more than linear.  The linear fit for these two variables is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y = -180.5674 + 0.3348trainingExample&lt;/script&gt;

&lt;p&gt;Nearly all of this additional time per training example comes from the time taken to fit the model.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/training_examples_totalTime.png&quot; alt=&quot;Training Examples&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;fitting-a-generalized-linear-model&quot;&gt;Fitting a Generalized Linear Model&lt;/h3&gt;
&lt;p&gt;Using the &lt;code class=&quot;highlighter-rouge&quot;&gt;glm&lt;/code&gt; function in R, I fitted a generalized linear model to the data, using all four predictors.  Using all the predictors, the model takes the form:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y = -176.0644 + 0.3343trainingExamples - 2.2136Cores + 2.2905GBMemory - 50.6004degree&lt;/script&gt;

&lt;p&gt;Using the Akaike Information Criterion (AIC) to evaluate the best model, I tried using different combinations of predictors. Using all four predictors, however, gives us the minimum AIC, so can be considered the best model out of all of the candidates.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;AIC = 66800&lt;/script&gt;

&lt;h3 id=&quot;evaluating-the-accuracy-of-the-glm&quot;&gt;Evaluating the Accuracy of the GLM&lt;/h3&gt;
&lt;p&gt;Using an independent testing set of 200 random experiments, I used the glm above to predict the total time.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/obs_pred.png&quot; alt=&quot;Prediction&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Unfortunately, our GLM doesn’t do a great job at predicting the testing set to the observed values.  Perhaps I’m forgetting a variable…&lt;/p&gt;

&lt;p&gt;We can also plot out the errors between observed (‘true’) values and predicted values.  Looking at the summary statistics, it appears my model will slightly under predict the total execution time.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/summary.png&quot; alt=&quot;SummaryStats&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;variance-within-cells&quot;&gt;Variance Within Cells&lt;/h3&gt;
&lt;p&gt;One of the things I was most worried about when starting this project was the within-cell variance that I would encounter due to internal computer variations and other concurrent processes.  Looking at the preliminary data, it appears that the variance within the cells does increase as the total experiment time increases.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/sd_mean.png&quot; alt=&quot;SD_Mean&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The linear model for standard deviation as a function of cell mean takes the form:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y = 15.7146 + 0.05261x&lt;/script&gt;

&lt;p&gt;Where x is the cell mean.  Note: A cell is combination of cores, memory, training examples, and spatial resolution, and each cell is computed ten times.&lt;/p&gt;

&lt;h3 id=&quot;computed-configurations&quot;&gt;Computed Configurations&lt;/h3&gt;

&lt;p&gt;At this time I’ve computed:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;vCPU&lt;/th&gt;
      &lt;th&gt;Number Completed&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1015&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;925&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;1435&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;1455&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;GB Memory&lt;/th&gt;
      &lt;th&gt;Number Completed&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;148&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;300&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;305&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;690&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;617&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;40&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;446&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;12&lt;/td&gt;
      &lt;td&gt;446&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;15&lt;/td&gt;
      &lt;td&gt;336&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;16&lt;/td&gt;
      &lt;td&gt;202&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;18&lt;/td&gt;
      &lt;td&gt;295&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;21&lt;/td&gt;
      &lt;td&gt;294&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;24&lt;/td&gt;
      &lt;td&gt;301&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;27&lt;/td&gt;
      &lt;td&gt;115&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;30&lt;/td&gt;
      &lt;td&gt;116&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;36&lt;/td&gt;
      &lt;td&gt;91&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;39&lt;/td&gt;
      &lt;td&gt;88&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
</description>
        <pubDate>Sun, 26 Jun 2016 09:22:52 -0500</pubDate>
        <link>/research/visualization/2016/06/26/preliminary-thesis-results.html</link>
        <guid isPermaLink="true">/research/visualization/2016/06/26/preliminary-thesis-results.html</guid>
        
        
        <category>Research</category>
        
        <category>Visualization</category>
        
      </item>
    
      <item>
        <title>Building the Niche Database and Web Services</title>
        <description>&lt;p&gt;I’ve spent some time over the past couple of weeks building out the Niche API, a set of web services that enable you to get global climate model (GCM) simulated climate data for specific points in space and time.  For this project I’ve been mixing database design, backend web programming, and a bit of cloud computing.  It’s been a fun process, and is turning into what I think will be a very useful tool.  In this post, I put down a few thoughts about the decisions I made, the techniques I used, and the problems I faced.&lt;/p&gt;

&lt;h3 id=&quot;the-challenge&quot;&gt;The Challenge&lt;/h3&gt;
&lt;p&gt;Working with GCM output and other gridded climate products is always challenging. The data is large.  It’s usually in a format that is designed for its transportation rather than its communication (NetCDF). It may be in the wrong projection for the work you need to do.  You might need to resample the spatial resolution of the grid to fit with the other layers in your project.  If you’ve ever done work with these datasets, you’re probably familiar with these obstacles.  An additional challenge that we face in paleoclimate/paleoecological projects is the many different time periods that are included in each dataset. In a NetCDF, the different timestamps are typically shipped as different layers.  However, if you start to unpack these layers, you’ll quickly end up with a tangled mess of rasters describing different time periods.&lt;/p&gt;

&lt;p&gt;In many cases, we don’t need the whole gridded raster dataset for our work.  Maybe we need a small subset representing a study site, or very often, we just want to know the value of a variable at a discrete point in space and time.  In my work with species distribution models, I want to be able to ask the question: What was the precipitation (or maximum temperature, or annual mean temperature, etc) at the coordinates of a sample in the Neotoma database at the time when the sample was dated to.  Traditionally, I would need to manage all sorts of datasets, and use ArcMap or some other utility to get the datapoint I need.&lt;/p&gt;

&lt;h3 id=&quot;the-goal&quot;&gt;The Goal&lt;/h3&gt;
&lt;p&gt;My goal with this project is to build a web service that does the management and extraction of climate datasets automatically. My objects include:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Store and manage gridded climate datasets in a way that preserves their metadata and makes them available via the internet&lt;/li&gt;
  &lt;li&gt;Be able to pass geographic coordinates and a calendar year and return an interpolated age for that space-time location&lt;/li&gt;
  &lt;li&gt;Be able to access the program scripts remotely through a web service&lt;/li&gt;
  &lt;li&gt;Make the platform generic enough to enable other users with other gridded datasets to contribute to the database&lt;/li&gt;
  &lt;li&gt;Maintain metadata on each dataset so that users can query for data by its attributes&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;My primary use case that I’m designing for is the &lt;a href=&quot;http://paleo.geography.wisc.edu&quot;&gt;NicheViewer&lt;/a&gt;, which plots samples in Neotoma on environmental axes.  This requires that the program be relatively fast (I’m still working on this) and return in a web-digestable format (JSON).  I’ve made some decision along the way that really reflect having NicheViewer as my target user, but at the &lt;a href=&quot;http://github.com/cyber4paleo&quot;&gt;C4P Hackathon&lt;/a&gt; in Boulder, CO this weekend, people showed significant enthusiasm for the project, and I’ll try to keep these other users in mind going forward.&lt;/p&gt;

&lt;h3 id=&quot;the-database&quot;&gt;The Database&lt;/h3&gt;
&lt;p&gt;Climate datasets are typically stored at NetCDF objects, a format which has been optimized for the sharing and storage of gridded data.  These files are self describing, with all the metadata needed to use them included in a header.  However, they’re difficult to extract data subsets from. Instead of relying on the NetCDF file type, I am going to the store the climate data as tables in a relational database.&lt;br /&gt;
Postgres is a relational database management system that has a great collection of spatial types included in the PostGIS extension.  With this extension, postgres has the ability to store large gridded datasets and to query them spatially.  It also includes the ability to store lines, polygons, multipolygons, and many other types of geometry if you want. Spatial queries are perfect for my task, because I can ask for the value of the raster at a latitude/longitude point.  I could also ask things like which cells are crossed by a line, or included in a polygon, but we’ll stick with points for now.&lt;/p&gt;

&lt;h4 id=&quot;raster-input-to-database&quot;&gt;Raster Input to Database&lt;/h4&gt;
&lt;p&gt;Getting rasters into the database is pretty easy, if you have them in a supported format (which NetCDF is not).  The workflow here was to first convert each band of the NetCDF (they’re not really bands, in this case they’re the discrete time/month/variable combinations) to a tiff file.  Then, using the &lt;code class=&quot;highlighter-rouge&quot;&gt;raster2pgsql&lt;/code&gt; tool included with postgres, I’ll then convert each image file to a SQL table of the Binary Large Object (BLOb) type.  Finally, using the &lt;code class=&quot;highlighter-rouge&quot;&gt;psql&lt;/code&gt; command shell, I can create a new table for this dataset in my dataset. I’ll typically do this in a python script that loops over all files in a directory.  Once I have a directory of tif files, I can do something like:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;os&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;subprocess&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;psycopg2&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# connect to the database&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;connectString&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;dbname=&#39;&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;database&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&#39; user=&#39;&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;username&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&#39; host=&#39;&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;host&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&#39; password=&#39;&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;password&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&#39;&quot;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;conn&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;psycopg2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;connect&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;connectString&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;cursor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;conn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cursor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# look through my local directory&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;basePath&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;/my/dir/of/files&quot;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;listdir&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;/my/dir/of/files&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# get the file name&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;fullName&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;/&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;basePath&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;tableName&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;RandomStringOfLettersAndNumbers&quot;&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# build the command&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;command&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;raster2pgsql &quot;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;command&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot; -s 4326&quot;&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;##wgs 1984 coordinate system&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;command&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot; -d&quot;&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;## overwrite this table name if it already exits&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;command&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot; -I&quot;&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;##spatial index&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;command&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot; -C &quot;&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;##enforce constraints&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;#command += &quot; -M &quot; ##vaccuum analyze&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;command&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rasterOnDisk&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;command&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot; -t 5x5 &quot;&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;##pixels per tile, the smaller the better&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;command&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tableName&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;##&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# run the command we&#39;ve just built&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;process&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;subprocess&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Popen&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;command&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stdout&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subprocess&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;PIPE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shell&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sql&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;err&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;process&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;communicate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# run the SQL through the database connection&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;cursor&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;execute&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sql&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# now the table is in the database&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4 id=&quot;table-metadata&quot;&gt;Table Metadata&lt;/h4&gt;
&lt;p&gt;The only way a database like this can succeed is if there is sufficient metadata to allow users to search and filter based on a layer’s metadata: &lt;em&gt;who produced this layer? what are its units? what variable does it represent?&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;My database keeps track of raster layer metadata in a set of postgres tables. These tables are divided into two types: those describing the model and/or entity that produced the dataset and those describing the variable described by the dataset.  I think this is a natural basis for the data model, because GCM models can produce multiple variable types, but each raster layer can have only one variable and one model source.  Similarly, each the details of each variable (units, measurement type, etc) can be the same for different time stamps, and for different climate model sources.&lt;/p&gt;

&lt;p&gt;The metadata is tracked using foreign keys from a central index table.  The index table stores a pointer to the actual data layer table, which is identified by a 64-bit unique identifier.  The random string is used as a table name to ensure that table names will be uniquely mapped to their metadata, even if a lot of tables get added to the database.  The index table also stores layer-specific metadata, such as the spatial resolution of the raster and the time period being represented, which do not map on to sources or variables.  Finally, the table has columns for variables and sources that are further described in those respective tables.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Source Metadata&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The &lt;em&gt;sources&lt;/em&gt; metadata table keeps track of the model and entity that produced the data layer in the first place. The sources table includes:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Producer:&lt;/em&gt; The name of the modeling entity or research group that produced the layer set, (e.g. National Center for Atmospheric Research [NCAR])&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Model:&lt;/em&gt; The name of the model that produced the layer set, (e.g. Community System Climate Model [CCSM])&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;ModelVersion:&lt;/em&gt; The version of the model that produced the layer set (e.g. 3.0),&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Scenario:&lt;/em&gt; The forcing scenario under which the model was run (e.g., UN IPCC RCP8.5),&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;ProductURL:&lt;/em&gt; The web address of the citation of the model, ideally a permanent &lt;a href=&quot;https://www.doi.org/&quot;&gt;doi&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Variables Metadata&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The &lt;em&gt;variables&lt;/em&gt; tables keep track of the thing that is being modeled.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;variableType:&lt;/em&gt; The climate variable being modeled, (e.g. Precipitation)&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;variableUnits:&lt;/em&gt; The units in which the variable is measured (e.g. centimeters [cm])&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;variablePeriod:&lt;/em&gt; The time frame over which the variable was measured, as represented by the layer (e.g., January [1])&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;variablePeriodType:&lt;/em&gt; The type of measuring time frame (e.g. Month, Quarter, Annual)&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;averagingPeriod:&lt;/em&gt;  The length of time over which the variable has been averaged, (e.g. 10)&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;averagingPeriodType:&lt;/em&gt;  A description of the units of time describing the averaging period (e.g. Years)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These properties can be combined together such that there are many combinations, and detailed metadata can still be kept track of.  The structure is such that a SQL query can enable a machine API to query and search the variables layers.  Each layer must have all properties specified.&lt;/p&gt;

&lt;p&gt;One of the things I am looking at implementing is coming closer to the &lt;a href=&quot;http://cfconventions.org/cf-conventions/v1.6.0/cf-conventions.html&quot;&gt;cf metadata standard&lt;/a&gt; that describes NetCDF files and already has a controlled vocabulary for climate layers.  I also want to change the variablePeriod implementation to reflect finer variations in represented time period, such as the difference between DJF and JFM.&lt;/p&gt;

&lt;h3 id=&quot;the-api&quot;&gt;The API&lt;/h3&gt;
&lt;p&gt;Once the database is in place, and has some data in it, we can enable programmatic access to its contents using an API. I have the both the database and the API running on a Google Compute Engine server running Debian Linux in the cloud.  This has proved really useful.  The API currently has way too many endpoints – you can add, modify, or delete nearly anything in the database – but that makes it really scalable if others want to start using it later on.  It’s been pretty RESTfully designed, and makes full use of the HTTP verbs.  The API is public, and the latest version of the documentation is &lt;a href=&quot;http://paleo.geography.wisc.edu/docs/&quot;&gt;here&lt;/a&gt;.  The full YAML declarations file is &lt;a href=&quot;http://paleo.geography.wisc.edu/docs/api_version_1_swagger.yaml&quot;&gt;here&lt;/a&gt;.  The API is &lt;a href=&quot;http://130.211.157.239:8080/&quot;&gt;served from here&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&quot;modeling-the-api&quot;&gt;Modeling the API&lt;/h4&gt;
&lt;p&gt;I wanted to be very methodical and document my progress in the development of this project, so I decided to use a modeling language that records the input and output properties of each endpoint.  I used the &lt;a href=&quot;http://editor.swagger.io/#/&quot;&gt;swagger&lt;/a&gt; modeling language and editor, which let me produce documentation and really think through my decisions before I started coding the application.  It’s designed to produce good documentation and is capable of generating client side code stubs for different languages which is kind of nifty.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;HTTP request&lt;/th&gt;
      &lt;th&gt;Description&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;DELETE&lt;/strong&gt; /averagingTypes/{averagingTypeID}&lt;/td&gt;
      &lt;td&gt;Delete an averaging type using its averagingTypeID&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;GET&lt;/strong&gt; /averagingTypes/{averagingTypeID}&lt;/td&gt;
      &lt;td&gt;Get details about a specific averaging type using its averagingTypeID&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;PUT&lt;/strong&gt; /averagingTypes/{averagingTypeID}&lt;/td&gt;
      &lt;td&gt;Update details of a specific averaging type using its averagingTypeID&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;GET&lt;/strong&gt; /averagingTypes&lt;/td&gt;
      &lt;td&gt;Get a list of the averaging types in the database&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;POST&lt;/strong&gt; /averagingTypes&lt;/td&gt;
      &lt;td&gt;Add a new averaging period to the database&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;GET&lt;/strong&gt; /data&lt;/td&gt;
      &lt;td&gt;Get the value of one or more layers at a space-time location&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;GET&lt;/strong&gt; /layers&lt;/td&gt;
      &lt;td&gt;Get a list of the layers in the database&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;DELETE&lt;/strong&gt; /layers/{layerID}&lt;/td&gt;
      &lt;td&gt;Delete a layer and its raster table using its layerID&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;GET&lt;/strong&gt; /layers/{layerID}&lt;/td&gt;
      &lt;td&gt;Get details about a specific layer using its layerID&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;PUT&lt;/strong&gt; /layers/{layerID}&lt;/td&gt;
      &lt;td&gt;Update a layer&#39;s details using its layerID&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;POST&lt;/strong&gt; /layers&lt;/td&gt;
      &lt;td&gt;Add a layer to the databases&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;GET&lt;/strong&gt; /sources&lt;/td&gt;
      &lt;td&gt;Get a list of the data sources and models in the database&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;POST&lt;/strong&gt; /sources&lt;/td&gt;
      &lt;td&gt;Add a new source to the database.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;DELETE&lt;/strong&gt; /sources/{sourceID}&lt;/td&gt;
      &lt;td&gt;Delete a source using its souce id&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;GET&lt;/strong&gt; /sources/{sourceID}&lt;/td&gt;
      &lt;td&gt;Get details about an specific source using its sourceID&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;PUT&lt;/strong&gt; /sources/{sourceID}&lt;/td&gt;
      &lt;td&gt;Update details about a specific source using its sourceID&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;GET&lt;/strong&gt; /variableUnits&lt;/td&gt;
      &lt;td&gt;Get a list of variable units in the database&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;POST&lt;/strong&gt; /variableUnits&lt;/td&gt;
      &lt;td&gt;Add a new variable unit to the database&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;DELETE&lt;/strong&gt; /variableUnits/{variableUnitID}&lt;/td&gt;
      &lt;td&gt;Delete a variable unit using its database id&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;GET&lt;/strong&gt; /variableUnits/{variableUnitID}&lt;/td&gt;
      &lt;td&gt;Get details about a specific variable unit instance&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;PUT&lt;/strong&gt; /variableUnits/{variableUnitID}&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;GET&lt;/strong&gt; /variablePeriodTypes&lt;/td&gt;
      &lt;td&gt;Get a list of the variable period types in the database.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;POST&lt;/strong&gt; /variablePeriodTypes&lt;/td&gt;
      &lt;td&gt;Add a new variable period type to the database&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;DELETE&lt;/strong&gt; /variablePeriodTypes/{variablePeriodTypeID}&lt;/td&gt;
      &lt;td&gt;Delete an variable period instance using its variablePeriodTypeID&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;GET&lt;/strong&gt; /variablePeriodTypes/{variablePeriodTypeID}&lt;/td&gt;
      &lt;td&gt;Get a details about a specific variable period type using its variablePeriodTypeID.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;PUT&lt;/strong&gt; /variablePeriodTypes/{variablePeriodTypeID}&lt;/td&gt;
      &lt;td&gt;Update the details of a specific variable period using its variablePeriodTypeID&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;GET&lt;/strong&gt; /variableTypes&lt;/td&gt;
      &lt;td&gt;Get a list of variable types in the database.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;POST&lt;/strong&gt; /variableTypes&lt;/td&gt;
      &lt;td&gt;Add a new variable type to the database&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;DELETE&lt;/strong&gt; /variableTypes/{variableTypeID}&lt;/td&gt;
      &lt;td&gt;Delete a variable type instance using its variableID&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;GET&lt;/strong&gt; /variableTypes/{variableTypeID}&lt;/td&gt;
      &lt;td&gt;Get details about a specific variable type&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;PUT&lt;/strong&gt; /variableTypes/{variableTypeID}&lt;/td&gt;
      &lt;td&gt;Update details about a specific variable type in the database&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;GET&lt;/strong&gt; /variables&lt;/td&gt;
      &lt;td&gt;List Niche Variables&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;POST&lt;/strong&gt; /variables&lt;/td&gt;
      &lt;td&gt;Add a new niche variable&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;DELETE&lt;/strong&gt; /variables/{variableID}&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;GET&lt;/strong&gt; /variables/{variableID}&lt;/td&gt;
      &lt;td&gt;Get details about a specific variable&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;PUT&lt;/strong&gt; /variables/{variableID}&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h4 id=&quot;writing-the-api&quot;&gt;Writing the API&lt;/h4&gt;
&lt;p&gt;Once modeled, the API script was written in python using the &lt;a href=&quot;http://bottlepy.org/docs/dev/index.html&quot;&gt;bottle&lt;/a&gt; web framework and a paste python-based web server.  Eventually it will be migrated to a windows server housed in our lab, and run behind an apache web server as a CGI module.  Basically, all the API does is receive HTTP requests, route them to functions, then the functions make SQL queries, and return the results as JSON. Pretty simple.  As you can see, there are a lot of endpoints included on the API right now, but the most important one is the &lt;em&gt;data&lt;/em&gt; endpoint, where you can actually get data from the database.&lt;/p&gt;

&lt;p&gt;The server runs forever using the &lt;code class=&quot;highlighter-rouge&quot;&gt;supervisor&lt;/code&gt; linux module (see my post on long running linux programs).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Making a request&lt;/strong&gt;:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;bottle&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;route&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;template&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;request&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;post&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;delete&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;put&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hook&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;psycopg2&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;## database connector&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;datetime&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;## for timestamp formatting&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;json&lt;/span&gt;


&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;JSONResponse&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;## base response class that returns the json fields I want&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;success&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;message&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;status&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;200&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;timestamp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;auto&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
            &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sucess&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;success&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;message&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;message&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;status&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;status&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;timestamp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;auto&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;timestamp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;datetime&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;datetime&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;now&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;strftime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Y-&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;m-&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;d &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;H:&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;M:&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;S&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;timestamp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;timestamp&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;toJSON&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;## just report the class&#39;s fields as a dictionary that will get converted to real json when bottle returns it&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__dict__&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;connectToDefaultDatabase&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&#39;&#39;&#39;Connect to the database using the settings configured in conf.txt&#39;&#39;&#39;&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;## read from the conf.txt file&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;conf.txt&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;r&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;header&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;hostname&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;password&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;user&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;dbname&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{}&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;line&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;fieldname&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;header&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;line&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;line&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;replace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;line&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;line&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;replace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&#39;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fieldname&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;line&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;hostname&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;hostname&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;db&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;dbname&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;pw&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;password&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;user&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;user&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;conn&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;psycopg2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;connect&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;host&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hostname&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;user&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;user&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;database&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;db&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;password&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pw&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;conn&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# now we can start making requests&lt;/span&gt;
&lt;span class=&quot;nd&quot;&gt;@get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;/someroute&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;doGetStuff&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# do some get stuff here!&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# the query parameters are stored in request.query&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;request&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;query&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;JSONResponse&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;#  make it serve&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;server&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;paste&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;host&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;0.0.0.0&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;port&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8080&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;debug&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Logging&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I want to be able to see who calls the API, which endpoints they call, and what kind of tech they’re using to call it.  I set up request logging on every request into a table called &lt;code class=&quot;highlighter-rouge&quot;&gt;call_log&lt;/code&gt;.  I accomplish this using a &lt;code class=&quot;highlighter-rouge&quot;&gt;hook&lt;/code&gt; which is fired before every request is passed on to its respective routing function, and basically just creates an apache style server log.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;c&quot;&gt;# this is the route that happens every time a request to anything is made&lt;/span&gt;
&lt;span class=&quot;nd&quot;&gt;@hook&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;before_request&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;log_this_request&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;logRequest&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;request&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;## this function deconstructs the request and puts it into a table&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;logRequest&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;req&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;con&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;connectToDefaultDatabase&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;cursor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;con&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cursor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;reqEnv&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;req&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;environ&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;method&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reqEnv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;REQUEST_METHOD&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;server_protocol&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reqEnv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;SERVER_PROTOCOL&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;user_agent&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reqEnv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;HTTP_USER_AGENT&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;remote_ip&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reqEnv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;REMOTE_ADDR&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;path&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;req&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;args&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;req&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;query_string&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;sql&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;&#39;&#39;INSERT INTO call_log VALUES(default, &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(resource)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s, &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(method)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s, &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(server_protocol)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s, &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(user_agent)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s, &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(remote_ip)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s, &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(args)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s, default);&#39;&#39;&#39;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&#39;method&#39;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;method&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&#39;server_protocol&#39;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;server_protocol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&#39;user_agent&#39;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;user_agent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&#39;remote_ip&#39;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;remote_ip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&#39;resource&#39;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&#39;args&#39;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;cursor&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;execute&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sql&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;con&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;commit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;cursor&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;close&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;con&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;close&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;CORS&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;To make post requests from the server where NicheViewer lives, I also needed to enable CORS.  CORS is always a pain in the ass, but its handled in the API like this:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;c&quot;&gt;# define the headers&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;_allow_origin&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;*&#39;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;_allow_methods&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;PUT, GET, POST, DELETE, OPTIONS&#39;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;_allow_headers&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;Authorization, Origin, Accept, Content-Type, X-Requested-With&#39;&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# attach them after every response&lt;/span&gt;
&lt;span class=&quot;nd&quot;&gt;@hook&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;after_request&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;enable_cors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&#39;&#39;&#39;Add headers to enable CORS&#39;&#39;&#39;&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;headers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;Access-Control-Allow-Origin&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_allow_origin&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;headers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;Access-Control-Allow-Methods&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_allow_methods&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;headers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;Access-Control-Allow-Headers&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_allow_headers&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# if the browser sends an Options request, let it know that it&#39;s safe to send over a post request&lt;/span&gt;
&lt;span class=&quot;nd&quot;&gt;@route&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;/data&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;method&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;OPTIONS&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;returnOptions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bottle&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;HTTPResponse&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;status&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;200&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;__&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Interpolating the Ages&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;One of the biggest challenges for me was getting the correct query to interpolate variables between ages.  The code ended up looking like this (yes, it is convoluted):&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;c&quot;&gt;#...import stuff and set up the connections&lt;/span&gt;

&lt;span class=&quot;nd&quot;&gt;@get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;/data&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;getData&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
  &lt;span class=&quot;c&quot;&gt;#...collect query parameters&lt;/span&gt;
  &lt;span class=&quot;c&quot;&gt;#...get a new DB cursor&lt;/span&gt;
  &lt;span class=&quot;c&quot;&gt;## fetch the table names that match our query&lt;/span&gt;
  &lt;span class=&quot;c&quot;&gt;# this query gets the tables and associated metadata that meet the user&#39;s query that have an age OLDER than the query year.&lt;/span&gt;
   &lt;span class=&quot;n&quot;&gt;greaterThanQuery&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;&#39;&#39;
       SELECT
           &quot;tableName&quot;, rasterIndex.yearsBP, sources.sourceID, sources.model, sources.producer, sources.productVersion, variables.variableID,
           variables.variabledescription, variableTypes.variableType, variableUnits.variableUnitAbbreviation, variables.variablePeriod,
           variablePeriodTypes.variablePeriodType, variables.variableAveraging, averagingPeriodTypes.averagingPeriodType, sources.producturl
       from rasterIndex
       INNER JOIN variables on rasterIndex.variableID=variables.variableID
       INNER JOIN    sources on rasterIndex.sourceID = sources.sourceID
       INNER JOIN    variableTypes on variables.variableType = variableTypes.variableTypeID
       INNER JOIN    variableUnits on variables.variableUnits = variableUnits.variableUnitID
       INNER JOIN    averagingPeriodTypes on variables.variableAveragingType = averagingPeriodTypes.averagingPeriodTypeID
       INNER JOIN    variablePeriodTypes on variables.variablePeriodType = variablePeriodTypes.variablePeriodTypeID
       WHERE rasterIndex.recordID IN
           (SELECT MIN(recordID) FROM rasterIndex
                       WHERE 1 = 1
                       AND (&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(variableType)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s is NULL or &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(variableType)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s LIKE lower(variableTypes.variableTypeAbbreviation) )
                       AND (&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(variablePeriod)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s is NULL or &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(variablePeriod)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s = variables.variablePeriod )
                       AND (&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(variablePeriodType)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s is NULL or &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(variablePeriodType)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s LIKE lower(variablePeriodTypes.variablePeriodType) )
                       AND (&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(averagingPeriod)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s is NULL or &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(averagingPeriod)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s = variableAveraging )
                       AND (&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(averagingPeriodType)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s is NULL or &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(averagingPeriodType)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s LIKE lower(averagingPeriodTypes.averagingPeriodType) )
                       AND (&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(variableUnits)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s is NULL or &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(variableUnits)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s LIKE lower(variableUnits.variableUnitAbbreviation))
                       AND (&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(variableID)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s is NULL or &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(variableID)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s = variables.variableID)
                       AND (&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(sourceID)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s is NULL or &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(sourceID)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s = sources.sourceID)
                       AND (&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(resolution)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s is NULL or &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(resolution)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s = resolution)
                       AND (&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(modelName)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s is NULL or &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(modelName)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s LIKE lower(sources.model) )
                       AND (&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(sourceProducer)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s is NULL or &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(sourceProducer)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s LIKE lower(sources.producer) )
                       AND (&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(modelVersion)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s is NULL or &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(modelVersion)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s = sources.productVersion )
                       AND (&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(modelScenario)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s is NULL or &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(modelScenario)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s LIKE lower(scenario) )
                       AND (yearsBP &amp;gt;= &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(yearsBP)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s)
                       GROUP BY variableID
           )
       ORDER BY sourceID, variableID;
       &#39;&#39;&#39;&lt;/span&gt;
   &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
       &lt;span class=&quot;s&quot;&gt;&#39;variableType&#39;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;variableType&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
       &lt;span class=&quot;s&quot;&gt;&#39;variablePeriod&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;variablePeriod&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
       &lt;span class=&quot;s&quot;&gt;&#39;variablePeriodType&#39;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;variablePeriodType&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
       &lt;span class=&quot;s&quot;&gt;&#39;variableUnits&#39;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;variableUnits&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
       &lt;span class=&quot;s&quot;&gt;&#39;variableID&#39;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;variableID&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
       &lt;span class=&quot;s&quot;&gt;&#39;averagingPeriod&#39;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;averagingPeriod&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
       &lt;span class=&quot;s&quot;&gt;&#39;averagingPeriodType&#39;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;averagingPeriodType&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
       &lt;span class=&quot;s&quot;&gt;&#39;sourceID&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sourceID&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
       &lt;span class=&quot;s&quot;&gt;&#39;sourceProducer&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sourceProducer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
       &lt;span class=&quot;s&quot;&gt;&#39;modelName&#39;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;modelName&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
       &lt;span class=&quot;s&quot;&gt;&#39;modelVersion&#39;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;modelVersion&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
       &lt;span class=&quot;s&quot;&gt;&#39;resolution&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;resolution&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
       &lt;span class=&quot;s&quot;&gt;&#39;modelScenario&#39;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;modelScenario&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
       &lt;span class=&quot;s&quot;&gt;&#39;yearsBP&#39;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;yearsBP&lt;/span&gt;
   &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
   &lt;span class=&quot;n&quot;&gt;cursor&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;execute&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;greaterThanQuery&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;## execute this query&lt;/span&gt;
   &lt;span class=&quot;n&quot;&gt;greaterThanRows&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cursor&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fetchall&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;## get and store the request&lt;/span&gt;

   &lt;span class=&quot;c&quot;&gt;## THIS IS THE LESS THAN query&lt;/span&gt;
   &lt;span class=&quot;c&quot;&gt;##This is the same query, but gets tables YOUNGER than the query age.&lt;/span&gt;
   &lt;span class=&quot;n&quot;&gt;lessThanQuery&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;&#39;&#39;
           SELECT
               &quot;tableName&quot;, rasterIndex.yearsBP, sources.sourceID, sources.model, sources.producer, sources.productVersion, variables.variableID,
               variables.variabledescription, variableTypes.variableType, variableUnits.variableUnitAbbreviation, variables.variablePeriod,
               variablePeriodTypes.variablePeriodType, variables.variableAveraging, averagingPeriodTypes.averagingPeriodType, sources.producturl
           from rasterIndex
           INNER JOIN variables on rasterIndex.variableID=variables.variableID
           INNER JOIN    sources on rasterIndex.sourceID = sources.sourceID
           INNER JOIN    variableTypes on variables.variableType = variableTypes.variableTypeID
           INNER JOIN    variableUnits on variables.variableUnits = variableUnits.variableUnitID
           INNER JOIN    averagingPeriodTypes on variables.variableAveragingType = averagingPeriodTypes.averagingPeriodTypeID
           INNER JOIN    variablePeriodTypes on variables.variablePeriodType = variablePeriodTypes.variablePeriodTypeID
           WHERE rasterIndex.recordID IN
               (SELECT MIN(recordID) FROM rasterIndex
                           WHERE 1 = 1
                           AND (&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(variableType)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s is NULL or &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(variableType)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s LIKE lower(variableTypes.variableTypeAbbreviation) )
                           AND (&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(variablePeriod)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s is NULL or &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(variablePeriod)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s = variables.variablePeriod )
                           AND (&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(variablePeriodType)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s is NULL or &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(variablePeriodType)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s LIKE lower(variablePeriodTypes.variablePeriodType) )
                           AND (&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(averagingPeriod)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s is NULL or &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(averagingPeriod)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s = variableAveraging )
                           AND (&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(averagingPeriodType)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s is NULL or &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(averagingPeriodType)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s LIKE lower(averagingPeriodTypes.averagingPeriodType) )
                           AND (&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(variableUnits)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s is NULL or &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(variableUnits)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s LIKE lower(variableUnits.variableUnitAbbreviation))
                           AND (&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(variableID)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s is NULL or &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(variableID)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s = variables.variableID)
                           AND (&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(sourceID)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s is NULL or &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(sourceID)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s = sources.sourceID)
                           AND (&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(resolution)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s is NULL or &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(resolution)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s = resolution)
                           AND (&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(modelName)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s is NULL or &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(modelName)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s LIKE lower(sources.model) )
                           AND (&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(sourceProducer)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s is NULL or &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(sourceProducer)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s LIKE lower(sources.producer) )
                           AND (&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(modelVersion)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s is NULL or &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(modelVersion)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s = sources.productVersion )
                           AND (&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(modelScenario)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s is NULL or &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(modelScenario)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s LIKE lower(scenario) )
                           AND (yearsBP &amp;lt;= &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(yearsBP)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s)
                           GROUP BY variableID
               )
           ORDER BY sourceID, variableID;
           &#39;&#39;&#39;&lt;/span&gt;

   &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
       &lt;span class=&quot;s&quot;&gt;&#39;variableType&#39;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;variableType&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
       &lt;span class=&quot;s&quot;&gt;&#39;variablePeriod&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;variablePeriod&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
       &lt;span class=&quot;s&quot;&gt;&#39;variablePeriodType&#39;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;variablePeriodType&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
       &lt;span class=&quot;s&quot;&gt;&#39;variableUnits&#39;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;variableUnits&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
       &lt;span class=&quot;s&quot;&gt;&#39;variableID&#39;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;variableID&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
       &lt;span class=&quot;s&quot;&gt;&#39;averagingPeriod&#39;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;averagingPeriod&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
       &lt;span class=&quot;s&quot;&gt;&#39;averagingPeriodType&#39;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;averagingPeriodType&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
       &lt;span class=&quot;s&quot;&gt;&#39;sourceID&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sourceID&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
       &lt;span class=&quot;s&quot;&gt;&#39;sourceProducer&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sourceProducer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
       &lt;span class=&quot;s&quot;&gt;&#39;modelName&#39;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;modelName&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
       &lt;span class=&quot;s&quot;&gt;&#39;modelVersion&#39;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;modelVersion&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
       &lt;span class=&quot;s&quot;&gt;&#39;resolution&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;resolution&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
       &lt;span class=&quot;s&quot;&gt;&#39;modelScenario&#39;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;modelScenario&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
       &lt;span class=&quot;s&quot;&gt;&#39;yearsBP&#39;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;yearsBP&lt;/span&gt;
   &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
   &lt;span class=&quot;n&quot;&gt;cursor&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;execute&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lessThanQuery&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;## execute&lt;/span&gt;
   &lt;span class=&quot;n&quot;&gt;lessThanRows&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cursor&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fetchall&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

   &lt;span class=&quot;c&quot;&gt;## these are the fields that will be returned with the response&lt;/span&gt;
   &lt;span class=&quot;n&quot;&gt;header&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;tableName&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;yearsBP&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;sourceID&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Model&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Producer&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;ModelVersion&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;variableID&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
   &lt;span class=&quot;s&quot;&gt;&quot;VariableDescription&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;VariableType&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;variableUnits&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;variablePeriod&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;variablePeriodType&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;averagingPeriod&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;averagingPeriodType&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;dataCitation&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

   &lt;span class=&quot;c&quot;&gt;## start building the response&lt;/span&gt;
   &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
   &lt;span class=&quot;c&quot;&gt;## fetch the actual point data from each of the returned tables&lt;/span&gt;
   &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
   &lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;greaterThanRows&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
       &lt;span class=&quot;c&quot;&gt;## the row order should match so we can interpolate between the sets of values&lt;/span&gt;
       &lt;span class=&quot;n&quot;&gt;greaterRow&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;greaterThanRows&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
       &lt;span class=&quot;n&quot;&gt;lesserRow&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lessThanRows&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
       &lt;span class=&quot;k&quot;&gt;try&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
           &lt;span class=&quot;c&quot;&gt;## go get the actual values from the raster  &lt;/span&gt;
           &lt;span class=&quot;n&quot;&gt;greaterTable&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;greaterRow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
           &lt;span class=&quot;n&quot;&gt;greaterYear&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;greaterRow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
           &lt;span class=&quot;c&quot;&gt;## for the higher value&lt;/span&gt;
           &lt;span class=&quot;n&quot;&gt;greaterQuery&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;&#39;&#39;SELECT ST_Value(rast,ST_SetSRID(ST_MakePoint(&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(longitude)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s, &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(latitude)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s), 4326)) FROM public.&#39;&#39;&#39;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;greaterTable&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;&#39;&#39;
               WHERE ST_Intersects(rast, ST_SetSRID(ST_MakePoint(&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(longitude)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s, &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(latitude)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s), 4326));
           &#39;&#39;&#39;&lt;/span&gt;
           &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;latitude&#39;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;latitude&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;longitude&#39;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;longitude&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
           &lt;span class=&quot;n&quot;&gt;cursor&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;execute&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;greaterQuery&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
           &lt;span class=&quot;n&quot;&gt;greaterValue&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cursor&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fetchone&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
           &lt;span class=&quot;c&quot;&gt;## for the lesser value&lt;/span&gt;
           &lt;span class=&quot;n&quot;&gt;lesserTable&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lesserRow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
           &lt;span class=&quot;n&quot;&gt;lesserYear&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lesserRow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
           &lt;span class=&quot;n&quot;&gt;lesserQuery&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;&#39;&#39;SELECT ST_Value(rast,ST_SetSRID(ST_MakePoint(&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(longitude)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s, &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(latitude)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s), 4326)) FROM public.&#39;&#39;&#39;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lesserTable&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;&#39;&#39;
               WHERE ST_Intersects(rast, ST_SetSRID(ST_MakePoint(&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(longitude)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s, &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%(latitude)&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s), 4326));
           &#39;&#39;&#39;&lt;/span&gt;
           &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;latitude&#39;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;latitude&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;longitude&#39;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;longitude&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
           &lt;span class=&quot;n&quot;&gt;cursor&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;execute&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lesserQuery&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
           &lt;span class=&quot;n&quot;&gt;lesserValue&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cursor&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fetchone&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
           &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;greaterValue&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;## no results were returned, likely because point was outside of north america&lt;/span&gt;
               &lt;span class=&quot;n&quot;&gt;greaterValue&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
           &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
               &lt;span class=&quot;n&quot;&gt;greaterValue&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;greaterValue&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

           &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lesserValue&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;## no results were returned, likely because point was outside of north america&lt;/span&gt;
               &lt;span class=&quot;n&quot;&gt;lesserValue&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
           &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
               &lt;span class=&quot;n&quot;&gt;lesserValue&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lesserValue&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
           &lt;span class=&quot;c&quot;&gt;## now do the Interpolation&lt;/span&gt;
           &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lesserYear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;greaterYear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
           &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lesserValue&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;greaterValue&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
           &lt;span class=&quot;n&quot;&gt;predPoint&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;yearsBP&lt;/span&gt;
           &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Prediction point is &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;predPoint&lt;/span&gt;
           &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;interpolate&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;interp1d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
           &lt;span class=&quot;n&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predPoint&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
           &lt;span class=&quot;n&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
           &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;numpy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;isnan&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
               &lt;span class=&quot;n&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;
           &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
           &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{}&lt;/span&gt;
          &lt;span class=&quot;c&quot;&gt;## add metadata about the table&lt;/span&gt;
           &lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;header&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
               &lt;span class=&quot;n&quot;&gt;col&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;header&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
               &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;col&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;greaterRow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
               &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
           &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;value&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;## this is the actual point value&lt;/span&gt;
           &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;latitude&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;latitude&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
           &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;longitude&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;longitude&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
           &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;siteName&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;siteName&lt;/span&gt;
           &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;sampleID&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sampleID&lt;/span&gt;
           &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;siteID&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;siteID&lt;/span&gt;
           &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;yearsBP&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;yearsBP&lt;/span&gt;
           &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
       &lt;span class=&quot;k&quot;&gt;except&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;Exception&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;## table doesn&#39;t exist, but record for table does exist --&amp;gt; oops&lt;/span&gt;
           &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
           &lt;span class=&quot;n&quot;&gt;conn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rollback&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
       &lt;span class=&quot;k&quot;&gt;finally&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
           &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
   &lt;span class=&quot;c&quot;&gt;## return the response&lt;/span&gt;
   &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;JSONResponse&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;message&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Linear interpolation between two nearest neighbors.&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;status&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;200&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
   &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bottle&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;HTTPResponse&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;status&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;200&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;body&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;toJSON&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4 id=&quot;api-responses&quot;&gt;API responses&lt;/h4&gt;

&lt;p&gt;The base response from the API includes these fields:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;status:&lt;/em&gt; HTTP status code, also included in the response header&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;timestamp:&lt;/em&gt; Time at which the response was minted by the server&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;message:&lt;/em&gt; A string produced by the server to specify an error, or note something of importance&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;success:&lt;/em&gt;  Boolean flag to indicate whether the API call was successful.  If success is &lt;code class=&quot;highlighter-rouge&quot;&gt;true&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;data&lt;/code&gt; is empty, then the user can be sure that the call happened correctly, there just was no matching data found in the database.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;data:&lt;/em&gt;  An array of objects that meet the search criteria&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Status Code&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;200:&lt;/em&gt; Success!&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;201:&lt;/em&gt; The requested object already exists, so the script didn’t create a new one&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;404:&lt;/em&gt; Object not found (resource doesn’t exist)&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;400:&lt;/em&gt; Required parameters for the method were not set&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;204:&lt;/em&gt; Object deleted&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;I believe the current solution described here meets all of the goals I set out to accomplish.  It’s a little hacky in places, and could be more robust, but for the most part it does everything it has to without error.  It currently supports two front ends: the NicheViewer and an R package.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Goals, evaluated&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Store and manage gridded climate datasets in a way that preserves their metadata and makes them available via the internet&lt;/em&gt;: Storing the raster layers in a postgres database makes it available over the internet.  While the metadata becomes divorced from the actual data, storing it in other tables keeps it close by, so we can refer to it if we need it.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Be able to pass geographic coordinates and a calendar year and return an interpolated age for that space-time location&lt;/em&gt;:  The API supports a SQL query that returns interpolated ages for spatial locations at a spatial location.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Be able to access the program scripts remotely through a web service:&lt;/em&gt;  The API supports RESTful querying of the data layers, so that user’s can get the data without installing software or downloading datasets.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Make the platform generic enough to enable other users with other gridded datasets to contribute to the database:&lt;/em&gt;  The table structure is generic enough to support (I think) any 2-dimensional gridded climate model output.  3-dimensional (height) datasets will not fit well into the data model, nor will datasets that are not gridded.  Datasets that are not modeled, but perhaps show real observations, should be able to fit into the database, though it hasn’t been tested.  The API needs a little work to support direct data ingestion, but version 1 supports public curation of metadata.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Maintain metadata on each dataset so that users can query for data by its attributes:&lt;/em&gt; The table structure maintains sufficient metadata on the model and the variable that is represented.  It is stored in several tables which allows it to be included in a structured query.  Version 1 of the API allows clients to query on all facets of what makes up a layer.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Future Work&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I have several things I would like to continue adding to this application.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Add more data layers, datasets, and models.&lt;/li&gt;
  &lt;li&gt;Improve API speed and robustness.&lt;/li&gt;
  &lt;li&gt;Let users query for other geometry types (lines, polygons, etc)&lt;/li&gt;
  &lt;li&gt;Improve metadata representations of time periods and variable types.&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Sun, 26 Jun 2016 08:22:52 -0500</pubDate>
        <link>/research/paleoclimate/2016/06/26/the-niche-api-web-service.html</link>
        <guid isPermaLink="true">/research/paleoclimate/2016/06/26/the-niche-api-web-service.html</guid>
        
        
        <category>Research</category>
        
        <category>Paleoclimate</category>
        
      </item>
    
      <item>
        <title>Automating my workflow: Building a (somewhat) distributed and (moderately) fault-tolerant system</title>
        <description>&lt;p&gt;I’ve made significant progress in getting a couple of Google’s computers to do my bidding (aka my thesis), in an automated way, so I thought I would share my experience setting up my cluster, and, specifically, the configuration of computing nodes and database/control nodes.  My setup draws on a bit on the design of larger systems like Hadoop, which create frameworks for massively large and distributed fault-tolerant systems.  In short, I have one Master Node that hosts a database and a control script, and a pool of compute nodes that are fault-tolerant and designed only for computing.  The compute nodes don’t have to know anything about the progress of the entire project and can handle being shut down mid-run, and the control node doesn’t have to know anything about the simulations being computed.&lt;/p&gt;

&lt;h3 id=&quot;requirements&quot;&gt;Requirements:&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;em&gt;Database:&lt;/em&gt; Needed to store the potentially large number of results obtained from the simulations&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Configurable Computers:&lt;/em&gt;  A main point of my thesis is that I can control the (virtual) hardware parameters of different computers and see how the simulation time responds.  Thus, I need a pool of computers with which I can control these parameters.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Automation:&lt;/em&gt; I’ve proposed the collection of ~84,400 different simulations on approximately 2,000 different hardware configurations. I don’t have the time or willpower to set all of these up manually, so I need a way to automate the process.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;API Access (optional):&lt;/em&gt; I am into API design and visualization, so I want to have an internet based method of getting the completion statistics for the project.  That way I can build a cool dashboard for the results and stuff, but this is last on the list of requirements.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;platform&quot;&gt;Platform:&lt;/h3&gt;
&lt;p&gt;I’ve decided to use the &lt;a href=&quot;http://cloud.google.com&quot;&gt;Google Cloud&lt;/a&gt; platform because of (a.) Its option for creating custom machine types that do not conform to predefined cloud computing servers and (b.) because of its free trial that allowed me to get a bunch more experiments in without the cost.  So far, my experience with google cloud has been overall positive, but not great.  There is an extensive amount of documentation on it, but it’s very dense and challenging if you don’t already have experience working within their frameworks.  There is only a limited amount of blogs/stackoverflows/etc to refer to if you encounter an error or problem.  On the other hand, there’s multiple ways of accessing your resources (console, REST api, command line), and several dashboards that give you a visualization of what’s happening.  So we’ve chosen the platform.&lt;/p&gt;

&lt;h3 id=&quot;approach&quot;&gt;Approach&lt;/h3&gt;
&lt;p&gt;Based on the need to many, many different computing configurations and the automation/database needs, I think it makes sense to split the virtual servers I’ll have on the cloud into two groups. At any one time I will have one or many servers that will actually be doing the job of computing the species distribution models and assessing their time and accuracy (compute nodes).  At the same time, I will have one server that hosts the database and the API, starts and stops the computing instances, and cleans up the workspace when necessary (Master Node). This approach allows me to use Google’s preemptible instances, which cost much less, but have the potential to be dropped due to system demand at any time.&lt;/p&gt;

&lt;h3 id=&quot;top-level-infrastructure&quot;&gt;Top-Level Infrastructure&lt;/h3&gt;
&lt;p&gt;The top-level infrastructure is composed of two parts: the central database and the Master Node.  In truly distributed systems, the system would not need access to a centralized database, instead each compute node would be able to do its own thing.  This strategy seemed like overkill and difficult to implement for this project, so I kept a single centralized database – hope it doesn’t crash.  I started out using Google’s CloudSQL, which was really a pain to set up, a pain to get data into, and a pain to get data out of.  So I stopped using it. Instead I run a small virtual server (f1-micro) so that I can SSH into it and not need the &lt;a href=&quot;% post_url 2016-6-2-adventures-in-google-cloud-I %&quot;&gt;very confusing&lt;/a&gt; CloudSQL Proxy required for I/O into the database.&lt;/p&gt;

&lt;p&gt;Also hosted on this small server, but conceptually different is a set of scripts that make up the ‘brain’ of the experiment. One of these scripts runs standard SQL queries against the database to determine the current position within the pool of experiments I want to accomplish.  This is the basis for the API, developed in node.js, and also the basis for the script that controls the setup and teardown of the compute nodes.&lt;/p&gt;

&lt;h4 id=&quot;configuring-and-building-the-virtual-instances&quot;&gt;Configuring and building the virtual instances&lt;/h4&gt;
&lt;p&gt;The steps to building and configuring the pool of computing nodes takes follows this general process:
1.  The MasterNode.py script uses the (daemonized [always running]) node.js web backend to query the database to ask “What experiments have not yet been marked as DONE?”.  The computing script could also mark experiments as “LEGACY”, “INTERRUPTED”, or “ERROR” depending on the conditions at runtime.  If they have not yet been computed, they are marked in the database as “NOT STARTED”. So MasterNode asks for everything that’s not “DONE”, and forces a re-compute if a simulation errored or was cut short.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;The central database, via the API, responds with a JSON object that contains the number of cores and memory needed for the next experiment (but not the other experimental parameters like number of training examples or spatial resolution).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;MasterNode parses the JSON and then uses the &lt;code class=&quot;highlighter-rouge&quot;&gt;gcloud&lt;/code&gt; tools to create a pool of computers that have the memory and number of cores specified by the database response.  This pool of virtual instances is automatically created with a startup script that installs the necessary software and files to run the computing experiments.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;running-the-simulations-reporting-the-results&quot;&gt;Running the simulations; Reporting the results&lt;/h4&gt;
&lt;p&gt;Now that I have the pool of virtual instances at my disposal, I can use them to run the SDM simulations, time their execution, and report back to the central database.  There are typically between 160 and 400 simulations to be done for every computing configuration (cores &amp;amp; memory), so on each node is an inner loop that looks like this:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Startup script installs git, mysql, and R.  Git clones the most recent version of the project repository which has all of the files needed for the computation.  R starts execution of the timeSDM.R script which controls the flow of execution for this node.&lt;/li&gt;
  &lt;li&gt;RScript queries the central database to ask “I am a compute of x cores and y GB memory, what experiments can I do?”.&lt;/li&gt;
  &lt;li&gt;The database responds with a single JSON row that contains all of the necessary parameters to actually run the SDM simulation (spatial resolution, taxon, number of training examples, etc).&lt;/li&gt;
  &lt;li&gt;RSCript parses the response and loads the correct set of variables, then runs the SDM model.
    &lt;ol&gt;
      &lt;li&gt;Fit the model (fitTime)&lt;/li&gt;
      &lt;li&gt;Project the model to AD 2100 (predictTime)&lt;/li&gt;
      &lt;li&gt;Evaluate accuracy (accuracyTime)&lt;/li&gt;
      &lt;li&gt;Return overall time, fitTime, predictTime, accuracyTime, and several measures of accuracy&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;RScript reports results back to the database.&lt;/li&gt;
  &lt;li&gt;Repeat until no experiments that are not “DONE” remain to be completed by a computer of this number cores and amount of memory.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;If an instance gets shut down due to preemption (or my incompetence) a shutdown script will be fired. This script records in the database that the experiment was cut off (INTERRUPTED) at some point before successful completion, and that it should be completed again in the future.&lt;/p&gt;

&lt;h4 id=&quot;managing-virtual-infrastructure&quot;&gt;Managing virtual infrastructure&lt;/h4&gt;
&lt;p&gt;Because Google charges you by the minute as you use their servers, and because I have to do a lot of different experiments and don’t have that much time do them, it is ideal to automatically tear down the servers and start a new pool as soon as one computing configuration has finished. So, while the computing nodes are doing their computing thing, the MasterNode is doing this:
1.  Repeated polling every 30 seconds:
    1.  MasterNode used the API to ask the database “What percentage of the experiments in this group have been completed?”
    2.  The database responds with a percentage (“DONE” / total)
2.  If the percentage is 100%, everything has been completed, so MasterNode will use &lt;code class=&quot;highlighter-rouge&quot;&gt;gcloud&lt;/code&gt; to delete the individual server instances, the instance group pool the they are part of, and the template used to create the instances.  After this, only the Master Compute Node server with the database on it still remains in my pool of Google resources.
3.  Repeat.  Configure and build a new pool of instances for the next memory/cores combination.&lt;/p&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;This method so far works pretty well.  Sometimes, the shutdown scripts don’t actually fire (there are known bugs), and so I have one or two experiments that are continuously marked as “STARTED”.  This is a problem for the MasterNode, because the database will continuously report something link 99.75% complete, but will never reach 100%.  When this happens, I need to go in and manually mark the session as closed and the experiments as INTERRUPTED, and then the normal flow of execution can continue.  So I have to remain watchful, but I don’t have to do everything.&lt;/p&gt;
</description>
        <pubDate>Thu, 16 Jun 2016 08:22:52 -0500</pubDate>
        <link>/research/cloudcomputing/2016/06/16/Computing-Configuration-Update.html</link>
        <guid isPermaLink="true">/research/cloudcomputing/2016/06/16/Computing-Configuration-Update.html</guid>
        
        
        <category>Research</category>
        
        <category>CloudComputing</category>
        
      </item>
    
      <item>
        <title>Managing Long-Running Processes on Linux</title>
        <description>&lt;p&gt;In my work, I have several times encountered the need to run a script for an extended period of time, or as a daemon (always running as a service).  Whether you’re on your own personal computer or SSHed into a virtual machine in the cloud, managing processes that take a long time can be annoying. If you finish your work day and close your laptop, you’re going to stop your script.  In the cloud (or I guess on a desktop/personal server too) you can take a couple steps to run scripts as services that will not stop when you end your work day.  There are a couple of ways of doing it that I’ve found.  Here are two that matched my needs.&lt;/p&gt;

&lt;h3 id=&quot;forever&quot;&gt;Forever&lt;/h3&gt;

&lt;h4 id=&quot;scenario&quot;&gt;Scenario:&lt;/h4&gt;
&lt;p&gt;I have a node.js based api server (to be discussed in a future post) that runs common sql queries for me so I can see the progress of my experiments over HTTP instead of typing out the commands manually.  I wrote the app, and can run it with &lt;code&gt;node app.js&lt;/code&gt;.  Works great – until I log off of SSH.  Because its a web server, I want it to be running all the time, so I can see it from anywhere on the internet, not just when I am actively working on my projects.  So I need a way to make this run automatically, 24/7.&lt;/p&gt;

&lt;h4 id=&quot;solution&quot;&gt;Solution:&lt;/h4&gt;
&lt;p&gt;Forever is a super simple tool designed to make your app run continuously. It is both written in node and designed to run node apps, so it’s a good choice for our scenario.  Also, it’s &lt;a href=&quot;https://github.com/foreverjs/forever&quot;&gt;open source&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;To install &lt;code&gt;forever&lt;/code&gt;, I will use the node package manager (npm).  &lt;code&gt;cd&lt;/code&gt; into your application’s project directory, then install the package with &lt;code&gt;[sudo] npm install forever -g&lt;/code&gt;.  The g flag should make the package available globally, and, importantly for us, available as a command line tool.  Y&lt;/p&gt;

&lt;p&gt;So I’ve got my app, I’ve debugged it both on a localhost and on the remote machine I’ll be hosting the server on, so I know it is going to work, and I’ve installed the package.  To start the process &lt;code&gt;cd&lt;/code&gt; into the directory you’re going to use, and then start the app with &lt;code&gt;forever [APPLICATION-NAME].js&lt;/code&gt;. In my case, it’s &lt;code&gt;forever app.js&lt;/code&gt;.  And that’s it.  You can check by examining the logs, or, more simply, going to wherever your application is serving content.  I have mine running on the 8080 port of the server I’m using as the master node / database of my project.  If I go there in my web browser, I see that my node project is running as expected.&lt;/p&gt;

&lt;p&gt;If you update your app, you’ll need to restart the service.  You can list the running services controlled by &lt;code&gt;forever&lt;/code&gt; by using &lt;code&gt;forever list&lt;/code&gt;.  Find your process (if you have more than one running), and then stop by &lt;code&gt; forever stop [ID]&lt;/code&gt;. If you have exactly one process running, you can shortcut the listing and just run &lt;code&gt;forever stop 0&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;In all, &lt;code&gt;forever&lt;/code&gt; is a very simple and easy to use solution for running node apps as services on virtual machines.  I think that we could use it to run scripts in other languages, like python, but I decided to use another tool for that.&lt;/p&gt;

&lt;h3 id=&quot;supervisor&quot;&gt;Supervisor&lt;/h3&gt;

&lt;h4 id=&quot;scenario-1&quot;&gt;Scenario:&lt;/h4&gt;
&lt;p&gt;I have a python script that I call MasterNode which controls the creation and deletion of all of the other virtual machines that I need to work with for my project.  MasterNode starts, figures out what the next core/memory combination is that has not be finished yet, then fires up a group of servers to do this job. When all of the experiments are complete, the script kills them, deletes them, and starts a new configuration.  The whole point of having MasterNode is that I don’t have to worry about specifying which configuration comes next, I can just sit back and relax while MasterNode figures it out for me.  This only works, though, if MasterNode is running, which stops after I log out of the virtual machine. I like sitting back and relaxing – So clearly I need a way to make it run continuously. I found a tool called &lt;code&gt;supervisor&lt;/code&gt; which is very similar to &lt;code&gt;forever&lt;/code&gt; but looks more robust and versatile.&lt;/p&gt;

&lt;h4 id=&quot;solution-1&quot;&gt;Solution:&lt;/h4&gt;
&lt;p&gt;Getting it up and running was slightly more challenging – &lt;a href=&quot;https://serversforhackers.com/monitoring-processes-with-supervisord&quot;&gt;this post&lt;/a&gt; was very helpful. To install &lt;code&gt;supervisor&lt;/code&gt; use &lt;code&gt;apt-get&lt;/code&gt;.  Specifically, we will use &lt;code class=&quot;highlighter-rouge&quot;&gt;sudo apt-get install -y supervisor&lt;/code&gt;.  The &lt;code class=&quot;highlighter-rouge&quot;&gt;-y&lt;/code&gt; flag indicates that you will answer yes to any question the system asks you (like ‘Are you sure you want to install this package?’).  Now we’ve installed the package, we need to define a program for it to run.&lt;/p&gt;

&lt;p&gt;Create a new program configuration file using &lt;code class=&quot;highlighter-rouge&quot;&gt;nano [APPLICATION-NAME].conf&lt;/code&gt;.  Inside of the file, you will write the details about where the program executable is located, whether it restarts, etc.  Details are confusing but can be found &lt;a href=&quot;http://supervisord.org/configuration.html&quot;&gt;here&lt;/a&gt;.  My configuration file looks like this: (comments are added, remove them)&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;table style=&quot;border-spacing: 0&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot; style=&quot;text-align: right&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;program:masterNode]  &lt;span class=&quot;c&quot;&gt;## name your process&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;command&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;python -u masterNode.py &lt;span class=&quot;c&quot;&gt;## this is the command that supervisor will use to start your script&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;directory&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/home/rstudio/thesis-scripts/python  &lt;span class=&quot;c&quot;&gt;## this is the directory that your script lives in&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;stdout_logfile&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/home/rstudio/thesis-scripts/logs/masterNode.log &lt;span class=&quot;c&quot;&gt;## this is where the output of your script will go&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;redirect_stderr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;true&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;## put errors into your log too&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Now, move the configuration file into the directory used by the supervisor program:&lt;/p&gt;

&lt;pre&gt;
sudo mv [APPLICATION-NAME].conf /etc/supervisor/conf.d
&lt;/pre&gt;

&lt;p&gt;The &lt;code class=&quot;highlighter-rouge&quot;&gt;conf.d&lt;/code&gt; directory holds all of the programs you want to be run by supervisor.  When it starts, it will look in this directory for any &lt;code class=&quot;highlighter-rouge&quot;&gt;.conf&lt;/code&gt; files.&lt;/p&gt;

&lt;p&gt;The first time you run the program, it might try to automatically start your program.  That’s cool, but if it doesn’t you can start your script running under supervisor with these steps:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Open the supervisor tool with &lt;code class=&quot;highlighter-rouge&quot;&gt;supervisorctl&lt;/code&gt;, which opens up a new supervisor shell.&lt;/li&gt;
  &lt;li&gt;Load any new configuration files with &lt;code class=&quot;highlighter-rouge&quot;&gt;reread&lt;/code&gt;, which will print out a list of available programs that you could run.&lt;/li&gt;
  &lt;li&gt;Start your program with &lt;code class=&quot;highlighter-rouge&quot;&gt;add [programName]&lt;/code&gt;, which will put your program under supervisor’s control.  Now you can sit back and relax :)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;You can review the logs of your scripts by looking at them directly in the folder where you put them, or better, you can view them in real time with &lt;code class=&quot;highlighter-rouge&quot;&gt;tail -f [programName]&lt;/code&gt; from the supervisor shell.&lt;/p&gt;

&lt;p&gt;To stop the program, you can &lt;code class=&quot;highlighter-rouge&quot;&gt;stop [programName]&lt;/code&gt;, which puts your application into paused mode. If you want to remove it from supervisor’s control all together, you can then enter &lt;code class=&quot;highlighter-rouge&quot;&gt;remove [programName]&lt;/code&gt; to take it off of supervisor’s list.&lt;/p&gt;

&lt;p&gt;This technique seemed to work well for me.  It auto-starts and auto-restarts your programs if they get cut off.  The package also comes with a web based manager on port 9001, which you can use to control your processes remotely.  Nifty.&lt;/p&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;There are many ways to control your long running processes.  One of the ones I did not mention here is to create the script as its own service.  However, I like the management and flexibility of the tools I discussed here.  Both of these tools would work to do either job too, so I think its really a matter of preference to pick one out.&lt;/p&gt;
</description>
        <pubDate>Sun, 12 Jun 2016 11:22:52 -0500</pubDate>
        <link>/research/cloudcomputing/2016/06/12/managing-long-running-linux-processes.html</link>
        <guid isPermaLink="true">/research/cloudcomputing/2016/06/12/managing-long-running-linux-processes.html</guid>
        
        
        <category>Research</category>
        
        <category>CloudComputing</category>
        
      </item>
    
      <item>
        <title>Startup and Shutdown Scripts in Google Cloud Compute Engine</title>
        <description>&lt;p&gt;In continuing my meditations on beginning to use the Google Cloud Computing platform, this post will describe the use of startup and shutdown scripts. If you want to start multiple instances that are all the same in terms of programs, data, etc (but perhaps of different size), you have two options. First, you could save your fully configured machine as an image, or more likely, as a snapshot.  Booting with this configuration is easy, just select the option from the menu when starting the new instance. Proceeding in this way has several potential drawbacks, however.  Most notably, it is very difficult to keep everything updated with this method.  Unless you manually update the snapshot pretty often, your software is going to be out of date.  Moreover, if you decide to make a small change in the scripts or programs you’re running on the instance, you will need to make an update to the snapshot.&lt;/p&gt;

&lt;p&gt;Instead of using a pre-configured snapshot, you can make use of Google Cloud’s ability to automatically run a script at boot time.  Using this script allows us to automatically install the latest version our programs using the &lt;code&gt;apt-get&lt;/code&gt; or other linux package manager, and if using version control software like git, download the latest working version of the scripts.  Google provides fairly easy-to-follow documentation about startup scripts &lt;a href=&quot;https://cloud.google.com/compute/docs/startupscript&quot;&gt;here&lt;/a&gt;.  In the same vein, Google provides a beta feature to run a script on machine termination, so you can save your work, etc.  There are a variety of limitations described in the &lt;a href=&quot;https://cloud.google.com/compute/docs/shutdownscript&quot;&gt;shutdown-script documentation&lt;/a&gt;, but they can be handy to run short cleanup jobs.&lt;/p&gt;

&lt;p&gt;As noted in the documentation, you can write the script in a number of languages by changing the shebang line at the top of the script.  To run a bash script, the first line of the script should be &lt;code&gt;#! /bin/bash&lt;/code&gt;.  A python script could be run by changing this line to &lt;code&gt;#!/usr/bin/python&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id=&quot;adding-a-script-to-a-single-instance&quot;&gt;Adding a script to a single instance&lt;/h3&gt;
&lt;p&gt;If you want to add a startup script to a single instance that is currently running:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Click on the instance properties in the Console&lt;/li&gt;
  &lt;li&gt;Click on edit, at the top&lt;/li&gt;
  &lt;li&gt;Scroll to the ‘Custom Metadata’ section&lt;/li&gt;
  &lt;li&gt;For key, enter &lt;code&gt;startup-script&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;In the value field, write your script&lt;/li&gt;
  &lt;li&gt;Click save at the bottom and restart the instance.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;adding-a-script-to-all-instances-in-your-project&quot;&gt;Adding a script to all instances in your project:&lt;/h3&gt;
&lt;p&gt;These items will be applied to all &lt;em&gt;new&lt;/em&gt; instances in your project.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Open the Cloud Computing Console&lt;/li&gt;
  &lt;li&gt;Click ‘Metadata’ in the lefthand navigation bar&lt;/li&gt;
  &lt;li&gt;Click ‘Edit’ at the top&lt;/li&gt;
  &lt;li&gt;Click ‘Add item’ to add a new key value pair&lt;/li&gt;
  &lt;li&gt;As above, for key enter &lt;code&gt;startup-script&lt;/code&gt;, and for value write your script&lt;/li&gt;
  &lt;li&gt;Save and close.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;It can be confusing sometimes because the metadata added here in this panel is automatically applied to all of your new instances.  You cannot see these key-value pairs in the metadata section of instance, so just remember it’s there.&lt;/p&gt;

&lt;h4 id=&quot;adding-a-shutdown-script&quot;&gt;Adding a shutdown script&lt;/h4&gt;
&lt;p&gt;Follow the same steps as to run a startup script, but instead of the &lt;code&gt;startup-script&lt;/code&gt; key, put &lt;code&gt;shutdown-script&lt;/code&gt;.  Super easy.  Be warned, though, that shutdown scripts are not guaranteed to run, Google completes them on a ‘best effort basis.’&lt;/p&gt;

&lt;h3 id=&quot;my-startup-script&quot;&gt;My Startup Script&lt;/h3&gt;
&lt;p&gt;I’ll describe my startup script and what I choose to do for each instance to set them up to run the experiments I am working with.  In general, I download, install, and update a bunch of software, I configure the google cloud proxy, and then I clone my github repository which contains all of my working files.  Then, when all is complete, I automatically start my experiments to run through an &lt;code&gt;Rscript&lt;/code&gt; and then terminate the machine when they’re complete.  The script is automatically run as root, from the root directory, so all of the &lt;code&gt;sudo&lt;/code&gt;’s are repetitive.&lt;/p&gt;

&lt;pre&gt;#! /bin/bash

--&amp;gt;Tell the computer I&#39;m working in a bash script

&lt;/pre&gt;

&lt;pre&gt;sudo apt-get update

--&amp;gt;Update the base packages that are installed on the instance
&lt;/pre&gt;

&lt;pre&gt;sudo apt-get -y install git
--&amp;gt;Install Git version control.  
&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;-y&lt;/code&gt; flag automatically answers &lt;code&gt;y&lt;/code&gt; to any queries presented by the download manager (i.e., are you sure you want to download this package? y/n).&lt;/p&gt;

&lt;pre&gt;sudo apt-get -y install r-base
--&amp;gt;Install R
&lt;/pre&gt;

&lt;pre&gt;sudo apt-get install -y gdebi-core
--&amp;gt;Install a utility dependency for use in installing RStudio
&lt;/pre&gt;

&lt;pre&gt;sudo apt-get install -y aptitude
--&amp;gt; Install the aptitude package manager
&lt;/pre&gt;

&lt;p&gt;Aptitude is a &lt;a href=&quot;http://askubuntu.com/questions/1743/is-aptitude-still-considered-superior-to-apt-get&quot;&gt;different&lt;/a&gt; package manager for linux systems.  This is the only one I could find that has a gdal package.&lt;/p&gt;

&lt;pre&gt;sudo aptitude install -y libgdal-dev
sudo aptitude install -y libproj-dev

--&amp;gt;Install gdal and proj4 libraries.
&lt;/pre&gt;

&lt;pre&gt;sudo apt-get install -y  libmariadb-client-lgpl-dev

--&amp;gt;Install a database dependency necessary for installing RMySQL
&lt;/pre&gt;

&lt;pre&gt;
wget https://download2.rstudio.org/rstudio-server-0.99.902-amd64.deb
sudo gdebi -y rstudio-server-0.99.902-amd64.deb

--&amp;gt; Download and install RStudio
&lt;/pre&gt;

&lt;pre&gt;
wget https://dl.google.com/cloudsql/cloud_sql_proxy.linux.amd64
mv cloud_sql_proxy.linux.amd64 cloud_sql_proxy
chmod +x cloud_sql_proxy

--&amp;gt;Download the cloud proxy script into your root directory and make it an executable.
&lt;/pre&gt;

&lt;pre&gt;
sudo mkdir cloudsql; sudo chmod 777 cloudsql

--&amp;gt;Make a directory for your cloudsql proxy sockets
&lt;/pre&gt;

&lt;pre&gt;
rm -rf /home/rstudio/thesis-scripts

--&amp;gt;Remove any old version of the script directory I&#39;ll be using.
&lt;/pre&gt;

&lt;pre&gt;git clone http://github.com/scottsfarley93/thesis-scripts /home/rstudio/thesis-scripts

--&amp;gt;Clone the latest and greatest version of my repo into my instance.
&lt;/pre&gt;

&lt;pre&gt;sudo ./cloud_sql_proxy  -dir=/cloudsql -instances=thesis-1329:us-central1:sdm-database-3 &amp;amp;

--&amp;gt;Start the cloudsql proxy so we can make database connections.
&lt;/pre&gt;

&lt;pre&gt;Rscript /home/rstudio/thesis-scripts/R/time_sdm.R 50 TRUE

--&amp;gt;Start the experiment script to run 50 iterations and then shutdown the machine.
&lt;/pre&gt;
</description>
        <pubDate>Sun, 05 Jun 2016 11:22:52 -0500</pubDate>
        <link>/research/cloudcomputing/2016/06/05/Startup-and-Shutdown-Scripts-GCE.html</link>
        <guid isPermaLink="true">/research/cloudcomputing/2016/06/05/Startup-and-Shutdown-Scripts-GCE.html</guid>
        
        
        <category>Research</category>
        
        <category>CloudComputing</category>
        
      </item>
    
      <item>
        <title>Connecting to Google Cloud SQL</title>
        <description>&lt;p&gt;Part of the reason that I am keeping this blog is to keep a record of the things I’ve done
and my thought process in doing them so that when it comes time to write up my thesis,
I will have a better recollection of what was going through my head. The other reason
is to perhaps help someone out there struggling similar problems that I went through.
I think that my adventures in the Google Cloud Platform are a good example of this –
Google’s Cloud Platform is acknowledged to be slightly less mature than some of its competitors, like AWS.
Because of this, there are fewer stackexchange questions, blogs posts, etc that can help guide
basic setup.  I do think that Google’s documentation and tutorials are better than Amazon’s –
more accessible, better written – but it can be hard to figure out what you need to
be doing if you’re not a cloud professional.  So I’ll document some of the hard steps
I encountered in this ongoing set of posts.&lt;/p&gt;

&lt;h3 id=&quot;setting-up-cloud-sql-with-mysql-workbench&quot;&gt;Setting up Cloud SQL with MySQL Workbench&lt;/h3&gt;
&lt;p&gt;In this post, I discuss the process of setting up Google’s Cloud SQL and getting it
to work with MySQL Workbench (the challenging part).  The tutorials on most of these
steps are pretty good, so I won’t try to completely recreate those.&lt;/p&gt;

&lt;h4 id=&quot;set-up-cloudsql&quot;&gt;Set up CloudSQL&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;Go to your Cloud Console&lt;/li&gt;
  &lt;li&gt;Click on SQL –&amp;gt; Go To SQL Dashboard&lt;/li&gt;
  &lt;li&gt;Click the Blue Plus button to start a new SQL Instance&lt;/li&gt;
  &lt;li&gt;Click the option to choose the second generation of SQL Servers.&lt;br /&gt;
These virtual machines are still in beta and Google hasn’t updated all of its documentation
about them yet, so some of the screenshots and instructions in the tutorials do not apply to the second generation
instances.&lt;/li&gt;
  &lt;li&gt;Walk through the wizard to create a new virtual machine instance for your database.&lt;/li&gt;
  &lt;li&gt;When you’re done with the wizard, click to create the VM and then wait for it to spin up.&lt;/li&gt;
  &lt;li&gt;Open the VM preferences, go to the Access Control tab, and then go to Users.
Change the root password to the password you want to use for logging into the server from a MySQL client.&lt;/li&gt;
  &lt;li&gt;Your database server should be set up and ready to go now.  You can access a SQL console and check its installation by clicking the button that looks like a terminal prompt in the upper righthand corner of the page.  Clicking that should open a command prompt.  Start the built-in MySQL client with:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;gcloud beta sql connect myinstance --user=root&lt;/pre&gt;
&lt;p&gt;If your server is up and running, you should see the terminal change to a &lt;code&gt;mysql&amp;gt;&lt;/code&gt; prompt.  If not, refer to &lt;a href=&quot;https://cloud.google.com/sql/docs/quickstart&quot;&gt;this walkthrough&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&quot;set-up-the-mysql-workbench&quot;&gt;Set up the MySQL Workbench&lt;/h4&gt;
&lt;p&gt;If you’re doing any time of analysis on the data stored within your database, you might be interested in working with an external tool like &lt;a href=&quot;https://www.mysql.com/products/workbench/&quot;&gt;MySQL Workbench&lt;/a&gt;.  I’m sure there are a variety of other good admin tools for MySQL, but this is the one I use and its okay.  This was a complicated part with the google cloud installation, and it took be the better part of a morning to work through the various tutorials to make it come together.  The real challenge is getting the CloudSQL Proxy set up. Because we’re using a second generation instance, the ip management is handled by Google Cloud and its CloudSQL Proxy directly, rather than manually by us.  Here we will assume you are using a Mac running OSX.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Get the Dependencies.  First, you need to get &lt;a href=&quot;http://rudix.org/packages/wget.html&quot;&gt;wget&lt;/a&gt; for your operating system.  Wget is a client for downloading files that is available on linux operating systems, but can be installed for windows and mac from third-parties.  Second, you need FUSE.  Not really sure what it does, but you need it and you can get it &lt;a href=&quot;https://sourceforge.net/projects/osxfuse/files/&quot;&gt;here&lt;/a&gt;.  I downloaded version &lt;code&gt;osxfuse-2.8.3 &lt;/code&gt; because it had 11,000 more downloads than any other version.&lt;/li&gt;
  &lt;li&gt;Download and configure your proxy script.  First &lt;code&gt;cd&lt;/code&gt; into your project root.
    &lt;pre&gt;
wget https://dl.google.com/cloudsql/cloud_sql_proxy.linux.amd64
mv cloud_sql_proxy.linux.amd64 cloud_sql_proxy
chmod +x cloud_sql_proxy
&lt;/pre&gt;
  &lt;/li&gt;
  &lt;li&gt;Configure your service account.
    &lt;ol&gt;
      &lt;li&gt;Go to the Cloud Console and select your projects&lt;/li&gt;
      &lt;li&gt;Click the button to Create Credentials&lt;/li&gt;
      &lt;li&gt;Choose Service Account Key&lt;/li&gt;
      &lt;li&gt;Choose to create a new Service Account&lt;/li&gt;
      &lt;li&gt;Proceed through the wizard and make sure that the key type is JSON&lt;/li&gt;
      &lt;li&gt;Create the key and store the automatically downloaded file somewhere safe on your computer.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;Setup a directory for the Proxy
    &lt;pre&gt;
sudo mkdir /cloudsql; sudo chmod 777 /cloudsql
&lt;/pre&gt;
  &lt;/li&gt;
  &lt;li&gt;Run the proxy by referencing the key file that your downloaded
    &lt;pre&gt;
sudo ./cloud_sql_proxy -dir=/cloudsql -fuse -credential_file=path/to/keyfile &amp;amp;
&lt;/pre&gt;
  &lt;/li&gt;
  &lt;li&gt;Start up MySQL Workbench
    &lt;ol&gt;
      &lt;li&gt;Find the Static IP address for your database server from the SQL instances console page on the Google Platform and copy it&lt;/li&gt;
      &lt;li&gt;Name the connection&lt;/li&gt;
      &lt;li&gt;Input the IP address but leave port 3306&lt;/li&gt;
      &lt;li&gt;Unless you changed the SQL, stick with root and be ready to enter the root password&lt;/li&gt;
      &lt;li&gt;Click test connection, enter your password, and you should be able to connect!!&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;repeated-use&quot;&gt;Repeated Use&lt;/h3&gt;
&lt;p&gt;I am sometimes able to reconnect with a stored connection in MySQL Workbench, but sometimes I get a &lt;code&gt;Refused to Connect&lt;/code&gt; error.  When this happened I just restarted the proxy with&lt;/p&gt;
&lt;pre&gt;sudo ./cloud_sql_proxy -dir=/cloudsql -fuse -credential_file=path/to/keyfile &amp;amp;&lt;/pre&gt;
&lt;p&gt;making sure I was in the same directory that I downloaded the cloudSQL proxy into originally.  After entering this command in the terminal, you should be able to connect to the database instance.&lt;/p&gt;

&lt;h3 id=&quot;on-the-compute-engine-server&quot;&gt;On the Compute Engine Server&lt;/h3&gt;
&lt;p&gt;If you’re also using the compute engine instances, you must go through a similar process of setting up the proxy on each one of your virtual machines before you can connect to the database server.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Before your create an instance that you plan to connect to the database server with, make sure that when you’re setting it up you give it  Full API access (or at least selectively enable the Cloud SQL API).  You can’t do this step later, you need to create a new virtual machine instance if you forget.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;SSH into your new computing instance.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Install mysql.  I always do my calls from python or R, but can’t hurt to install the mysql client on the new machine.  Its easier to test if things are going right too.
    &lt;pre&gt;
sudo apt-get update
sudo apt-get install mysql-client
&lt;/pre&gt;
  &lt;/li&gt;
  &lt;li&gt;Install the proxy like you did on your local computer.
    &lt;pre&gt;
wget https://dl.google.com/cloudsql/cloud_sql_proxy.linux.amd64
mv cloud_sql_proxy.linux.amd64 cloud_sql_proxy
chmod +x cloud_sql_proxy
&lt;/pre&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Find out what your database server’s connection name is.  It is listed on the sql instances page, and is something like [projectid]:[zone]:[instanceName].  It is not the compute engine instance name (tried that, didn’t work).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Start the proxy.  Probably want to be in a root directory for this.
    &lt;pre&gt;
sudo mkdir /cloudsql; sudo chmod 777 /cloudsql
sudo ./cloud_sql_proxy -dir=/cloudsql -instances=[INSTANCE_CONNECTION_NAME] &amp;amp;
&lt;/pre&gt;
    &lt;p&gt;with the [INSTANCE_CONNECTION_NAME] set to the connection name of your database server.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Start the MySQL
    &lt;pre&gt;
mysql -u root -p -S /cloudsql/[INSTANCE_CONNECTION_NAME]
&lt;/pre&gt;
    &lt;p&gt;Assuming everything worked as planned, you should now be able to see a &lt;code&gt;mysql&lt;/code&gt; prompt, and be able to play with your databases.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;connecting-from-code&quot;&gt;Connecting from Code&lt;/h3&gt;
&lt;p&gt;If you installed the proxy on your compute engine instance, you probably are also interested in doing some database manipulation from scripts.  For me, I was not able to connect via the &lt;code class=&quot;highlighter-rouge&quot;&gt;host&lt;/code&gt; parameter in the database connection functions, and instead needed to use a unix socket.&lt;/p&gt;

&lt;p&gt;These steps are done on your virtual machine.  SSH into it.  For you can use &lt;code class=&quot;highlighter-rouge&quot;&gt;nano&lt;/code&gt; or &lt;code class=&quot;highlighter-rouge&quot;&gt;vim&lt;/code&gt; or other text editor to do the steps in your script.  For R, you could use the rStudio server page.&lt;/p&gt;

&lt;h4 id=&quot;python&quot;&gt;Python&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;Install your favorite mysql connector module.
    &lt;pre&gt;
  sudo apt-get install mysqldb-python
&lt;/pre&gt;
  &lt;/li&gt;
  &lt;li&gt;In your script:&lt;/li&gt;
&lt;/ol&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;MySQLdb&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;con&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MySQLdb&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;connect&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unix_socket&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;/cloudsql/&#39;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;[INSTANCE_CONNECTION_NAME]&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;user&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;root&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;db&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;[your_database]&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;passwd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;[your_password]&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Connected to the database!&quot;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Proceed with scripting.&lt;/p&gt;

&lt;h4 id=&quot;r&quot;&gt;R&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;Install your favorite mysql connector package with the R package manager.
    &lt;pre&gt;
install.packages(&quot;RMySQL&quot;)
&lt;/pre&gt;
  &lt;/li&gt;
  &lt;li&gt;In your script, make the connection.&lt;/li&gt;
&lt;/ol&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;n&quot;&gt;library&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;RMySQL&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;## for database communication
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;drv&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dbDriver&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;MySQL&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;con&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dbConnect&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;drv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;unix.socket&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&#39;/cloudsql/[INSTANCE_CONNECTION_NAME]&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;username&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&#39;root&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;password&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&#39;[your_password]&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dbname&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&#39;[your_database]&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Proceed with scripting.&lt;/p&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;It’s definitely not easy, but after spending several hours at it, it sometimes works.&lt;/p&gt;
</description>
        <pubDate>Thu, 02 Jun 2016 11:22:52 -0500</pubDate>
        <link>/research/cloudcomputing/2016/06/02/adventures-in-google-cloud-I.html</link>
        <guid isPermaLink="true">/research/cloudcomputing/2016/06/02/adventures-in-google-cloud-I.html</guid>
        
        
        <category>Research</category>
        
        <category>CloudComputing</category>
        
      </item>
    
      <item>
        <title>On Finding Data for Cartography Projects, Part II: Spatial Data Services and APIs</title>
        <description>&lt;h3&gt;Using APIs and Data Services&lt;/h3&gt;
&lt;p&gt;This is the second installment in my series about finding data from new and different sources for use in your cartography or GIS projects.  Last time I discussed looking through existing source code to find hidden datasets that might be useful. Today, I will walk through using an API service to tap into an organization’s database.  As a simple google search will reveal, there are other resources, blogs, and tutorials out there that talk about how to use an API as a data source, but I will focus particularly on converting data from an API into a useful spatial data format that can be used in mapping and analysis.  Tons of APIs have spatial data (usually latitude and longitude) attached to their responses, its just a matter of finding the data service and massaging it into the right format.&lt;/p&gt;
&lt;h4&gt;What is an API?&lt;/h4&gt;
&lt;p&gt;An API, which stands for Application Programming Interface, is a set of protocols and methods that define how two computers should talk to each other.  An API is a documented set of building blocks (of code) that define how an existing application works.  A programmer can put these blocks together to extend the existing program, or create a new app that uses portions of the existing program.  Consider &lt;a href=&quot;http://twitter.com&quot;&gt;Twitter&lt;/a&gt;.  Twitter is super popular, and a lot of people use it for various things – documenting every facet of their daily lives, reporting news, &lt;a href=&quot;http://chrisscheele.com/&quot;&gt;observing disasters and severe weather&lt;/a&gt;, etc.  To build the platform, Twitter needed to make a whole bunch of computers talk to each other. When a user writes a tweet, it is sent to twitter’s central database, where it is stored, and then pushed back out to other clients.  Multiply this by Twitter’s &amp;gt;310 million users, both reading and writing tweets, and you have a lot of clients that need to communicate with minimal friction.&lt;/p&gt;

&lt;p&gt;Twitter could have kept the language that all of these clients and servers spoke to each other in a secret.  That’s what is called a private API.  Details of private APIs are not released to outside developers, but are (usually) documented and (sometimes) logical for the internal use of the organization or company that created it.  Sometimes they can be hacked (see Google Maps, before they released a public version of their API), but do not promote easy outside development. There are a lot of private APIs that support how your computer works, but we won’t talk about those today.&lt;/p&gt;

&lt;p&gt;Instead of keeping their API private, they released it to the world as a public API. Each method for user management, posting tweets, reading tweets, etc is documented and given with examples on &lt;a href=&quot;https://dev.twitter.com/overview/documentation&quot;&gt;twitter’s development website&lt;/a&gt;. Now any developer in the world can sign up with twitter and start posting and reading tweets through their own code.  If you’ve ever used TweetDeck or another twitter application that isn’t just the twitter app, its based on the public API.  Lots of companies build APIs so that developers outside of the organization can build apps on top of the company’s existing platform.&lt;/p&gt;
&lt;h4&gt;Using an API&lt;/h4&gt;
&lt;p&gt;So if I know a little coding, I should be able to tap into any existing public API in a few steps.  Basically, our process will be (1) build a query, (2) submit the query to the API, (3) get the result, (4) use the response.&lt;/p&gt;

&lt;p&gt;For the remainder of this post, I will use the &lt;a href=&quot;http://neotomadb.org&quot;&gt;Neotoma Paleoecological Database &lt;/a&gt;as an example, because (1) I think they have a well-designed, well-documented data service available through an API, and (2) I work on the Neotoma project. The Neotoma database aggregates and disseminates Quaternary fossil plant and animal data that support paleoecological research.  For this example, I want to make a list of fossil sites above 4,000 meters. The neotoma API docs are &lt;a href=&quot;http://api.neotomadb.org/doc&quot;&gt;here&lt;/a&gt;, and might be helpful for following along.&lt;/p&gt;

&lt;h4 id=&quot;api-organization&quot;&gt;&lt;em&gt;API Organization&lt;/em&gt;&lt;/h4&gt;
&lt;p&gt;An API is typically accessed through a web URL.  The URL is made up of a root, a resource, and a set of key-value pairs that define the parameters of your query. For Neotoma, the root of our query will be&lt;/p&gt;
&lt;pre&gt;http://api.neotomadb.org/v1/data&lt;/pre&gt;
&lt;p&gt;On top of the root, we need to specify an resource. The resource is the name of the data that you are trying to obtain.  This requires you to be a little familiar with the organization whose API you are using.  You can usually figure out which resource you want by browsing the API documentation.  For twitter, the resources include ‘friends’, ‘statuses’, ‘timeline’, etc.  In the case of Neotoma, we have ‘Sites’, ‘Taxa’, ‘Datasets’, ‘Downloads’, ‘SampleData’, ‘Publications’, ‘Contacts’, and ‘DBTables’.  Because I want a list of locations, I decide that I want to use the Sites endpoint, though if I want more detailed information, or the actual pollen counts, I might consider using the downloads or SampleData resources.  I add the resource on to my url string like so:&lt;/p&gt;
&lt;pre&gt;http://api.neotomadb.org/v1/data/sites&lt;/pre&gt;
&lt;p&gt;If I enter this query into my web browser, it will return every single site that Neotoma stores in its database (several thousand). While this can be useful in some scenarios, it is not what I want right now.  Instead, I want to filter down the result set to show only those sites above 4,000 meters.  I do this through using service parameters.  Each resource can have different parameters, and the parameters each resource has is determined by the developer of the API (not you). We see on the &lt;a href=&quot;http://api.neotomadb.org/doc/resources/sites&quot;&gt;Sites documentation page&lt;/a&gt;, that this resource accepts the parameters ‘sitename’, ‘altmin’, ‘altmax’, ‘loc’, and ‘gpid’.  All parameters are option, and are additive, so you can filter in really customizable ways.  Parameters are just added onto the query string:&lt;/p&gt;
&lt;pre&gt;?key1=value1&amp;amp;key2=value2&amp;amp;...&amp;amp;keyN=valueN&lt;/pre&gt;
&lt;p&gt;So our query string becomes&lt;/p&gt;
&lt;pre&gt;http://api.neotomadb.org/v1/data/sites?altmin=4000&lt;/pre&gt;
&lt;p&gt;When we enter this into the web browser, we see that the results set is much smaller.&lt;/p&gt;

&lt;h4 id=&quot;api-response&quot;&gt;&lt;em&gt;API Response&lt;/em&gt;&lt;/h4&gt;
&lt;p&gt;Every organizations API will return a different response, and every resource within an API can return a different response.  Of course, documentation can help you determine when you are looking at, but it can also be super help to just enter your desired query string into your web browser and look at what you are getting back.  Most APIs these days will return JSON formatted responses, though some will return geojson, csv, plain text, xml, or some other data type.  If you are getting a lot of JSON back, a pretty printer like the plug-in for &lt;a href=&quot;https://chrome.google.com/webstore/detail/jsonview/chklaanhfefbnpoihckbnefhakgolnmc?hl=en&quot;&gt;chrome&lt;/a&gt; can really make your life easier.&lt;/p&gt;

&lt;p&gt;In our Neotoma example, our response returned a big json object.  The top level keys are ‘success’ and ‘data’.  If success is false, or 0, you probably entered an invalid query string (specified a resource that does not exist, or gave a parameter that is not accepted), but might also be due to a server outage.  In this case there will also be a ‘message’ key that will tell you the reason that your call failed.  When success is true, or 1, you will get an array of json objects that each have the properties listed in the documentation page for that resource.  We see that our objects will have the properties: ‘SiteID’, ‘SiteName’, ‘LatitudeNorth’, ‘LatitudeSouth’, ‘LongitudeWest’, ‘LongitudeEast’, ‘SiteNotes’,’SiteDescription’, ‘Altitude’.&lt;/p&gt;

&lt;h4 id=&quot;implementing-an-api-call-javascript-and-ajax&quot;&gt;Implementing an API call: Javascript and AJAX&lt;/h4&gt;
&lt;p&gt;The next two section will demonstrate implementations of the API call that was developed above, the first in asynchronous javascript for use in a web application, and below, in python, to build a CSV that can be used in ArcMap or other projects.&lt;/p&gt;

&lt;p&gt;Here is an example of asking Neotoma for all of the sites above 4,000 meters.  The most important thing to remember is that this an asynchronous AJAX call, so it will take second to respond, and your code has to be able to handle this in its organization.  First we will build the query string, next we will send it to Neotoma using jQuery’s $.ajax function, and finally, we will deal will the response. Another important facet of using jQuery and javascript’s ajax technique is that you don’t have to build the response yourself, you can just pass in a &lt;code&gt;data&lt;/code&gt; parameter in the ajax call, and the string will be built automatically. You can still see the built query string by &lt;code&gt;console.log&lt;/code&gt;-ing the &lt;code&gt;this.url&lt;/code&gt; on &lt;code&gt;beforeSend&lt;/code&gt;.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-javascript&quot; data-lang=&quot;javascript&quot;&gt;&lt;table style=&quot;border-spacing: 0&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot; style=&quot;text-align: right&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;getNeotomaData&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;minAlt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
  &lt;span class=&quot;nx&quot;&gt;endpoint&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&#39;http://api.neotomadb.org/v1/data/sites&#39;&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;//this is the root and resource&lt;/span&gt;
  &lt;span class=&quot;nx&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;ajax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;endpoint&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;//make an ajax call with the query string url&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;dataType&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;jsonp&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;//its json, but coming from a remote server, so jsonp&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;beforeSend&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(){&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;//optional, but helpful for debugging&lt;/span&gt;
      &lt;span class=&quot;nx&quot;&gt;console&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;this&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;url&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;//to see exactly where the call is going to&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;
    &lt;span class=&quot;nx&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;nl&quot;&gt;altmin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;minAlt&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;//pass in the key-value parameters&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;
    &lt;span class=&quot;nx&quot;&gt;success&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
      &lt;span class=&quot;c1&quot;&gt;//called when the call succeeds&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&#39;success&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]){&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;//check whether the server said okay&lt;/span&gt;
        &lt;span class=&quot;nx&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&#39;data&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;//just take the data from the response&lt;/span&gt;
        &lt;span class=&quot;nx&quot;&gt;doStuffWithData&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;//callback function to proceed with the script&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;//the server threw an error, so check what it was&lt;/span&gt;
        &lt;span class=&quot;nx&quot;&gt;console&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Error on the API call.&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;nx&quot;&gt;console&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&#39;message&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;//the server will tell you what&#39;s wrong&lt;/span&gt;
      &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;
    &lt;span class=&quot;nx&quot;&gt;error&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;xhr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;status&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;error&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
      &lt;span class=&quot;c1&quot;&gt;//there was an AJAX error (communcation problem)&lt;/span&gt;
      &lt;span class=&quot;nx&quot;&gt;console&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;AJAX error.&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;nx&quot;&gt;console&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;error&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;doStuffWithData&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;dataArray&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
  &lt;span class=&quot;c1&quot;&gt;//do stuff here&lt;/span&gt;
  &lt;span class=&quot;c1&quot;&gt;//put the data on a map?&lt;/span&gt;
  &lt;span class=&quot;c1&quot;&gt;//make a table of the sites?&lt;/span&gt;
  &lt;span class=&quot;c1&quot;&gt;//do analysis?&lt;/span&gt;
  &lt;span class=&quot;c1&quot;&gt;//the world is your oyster&lt;/span&gt;
  &lt;span class=&quot;nx&quot;&gt;console&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;dataArray&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;//or just print the message&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;If you have a page with jQuery included on it and call &lt;code&gt;getNeotomaData&lt;/code&gt;, you should see the response get logged into the console.&lt;/p&gt;

&lt;p&gt;Pulling from an API in a web app is great because (1) You don’t have to store and maintain a file, and (2) you have access to updates whenever the organization’s database is updated.&lt;/p&gt;

&lt;h4 id=&quot;implementing-an-api-callpython-to-csv&quot;&gt;IMPLEMENTING AN API CALL: Python to CSV&lt;/h4&gt;
&lt;p&gt;If you’re not making an interactive map for a web app, you’re unlikely to be using javascript and AJAX, but you still might want to tap into the data service.  Here I will demonstrate a simple json to csv conversion script that calls the same API query in the examples above.&lt;/p&gt;

&lt;p&gt;There are some web-based tools to convert json to csv.  However, since JSON can be hierarchical and a csv is flat, it can be difficult for these tools to work correctly.  If you have some level of competency using python, I recommend custom-rolling your conversions each time you need to call a new resource, to make sure you get the fields that you need in your CSV.  This example uses the &lt;code&gt;csv&lt;/code&gt; module for writing the file and the &lt;code&gt;requests&lt;/code&gt; module for making the api call.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;table style=&quot;border-spacing: 0&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot; style=&quot;text-align: right&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;csv&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;requests&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;saveDataFromNeotoma&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;minAlt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;outfile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;## set up the output file&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;outfile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;w&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;## open the file buffer&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;fields&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;SiteID&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;SiteName&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;LatitudeNorth&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&#39;LatitudeSouth&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;LongitudeEast&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;LongitudeWest&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&#39;Altitude&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;SiteDescription&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;Notes&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;## fields to use as the header for the CSV&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;writer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;csv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DictWriter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fieldnames&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fields&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lineterminator&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;c&quot;&gt;## write a line when we pass a dictionary&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;writer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;writeheader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;## write the top header row&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;## make the query string&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;endpoint&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;http://api.neotomadb.org/v1/data/sites?&quot;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;url&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;endpoint&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;altmin=&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;minAlt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;c&quot;&gt;## this is the complete query string&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;try&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;response&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;requests&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;url&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;json&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;## make the call and parse the response as json&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;except&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;Exception&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;## there was a communication error&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Failed to reach the neotoma server.&quot;&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;nb&quot;&gt;exit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;## die&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;success&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;## there was an error on the call&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;There was a communication error&quot;&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;message&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;## this is the error message&lt;/span&gt;
        &lt;span class=&quot;nb&quot;&gt;exit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;data&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;site&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;## iterate through all the sites&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;## the fields in the header are all the same as the fields in the response objects&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;## so we can just write with the response objects&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;## otherwise, we could do more manipulation here&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;try&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;c&quot;&gt;# the encoding can be funky when writing to excel, so fix it&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;site&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;SiteName&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;site&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;SiteName&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;utf-8&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;site&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;SiteDescription&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;site&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;SiteDescription&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;utf-8&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;except&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;AttributeError&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;## site didn&#39;t have site description&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;pass&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;## don&#39;t worry about it&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;writer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;writerow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;site&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;## write the row&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# finish up&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;close&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;saveDataFromNeotoma&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;/path/to/your/intended/file.csv&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;In this way, you can create a file just like it had been made available for public downloading, but (1) you’ve done it on a resource that was not available as a file download, and (2) you were able to exactly configure the response how you want it, so you don’t have to mess around in excel filtering and sorting.&lt;/p&gt;

&lt;p&gt;Hopefully this post was helpful in getting you started on using APIs and data services, and that you can maybe use these techniques in your own work at some point.  Spatial data APIs are everywhere – happy hunting.&lt;/p&gt;
</description>
        <pubDate>Tue, 31 May 2016 11:22:52 -0500</pubDate>
        <link>/cartography/2016/05/31/Find-Data-For_Cartography-Projects-II.html</link>
        <guid isPermaLink="true">/cartography/2016/05/31/Find-Data-For_Cartography-Projects-II.html</guid>
        
        
        <category>cartography</category>
        
      </item>
    
  </channel>
</rss>
